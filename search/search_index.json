{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Services","text":"<p>If you are challenged with any of these areas, I can help:</p> <ul> <li>Understanding AI capabilities and limitations: What is possible now and in the near future.</li> <li>Prompt Engineering: Techniques to craft effective prompts for Generative AI.</li> <li>Retrieval-Augmented Generation (RAG): Implementing RAG with hybrid vector and semantic search with re-ranking. Including text, tabular data and images. Evaluating Embedding models.</li> <li>Generative AI hallucination: Understanding how to reduce and prevent hallucination.</li> <li>Evals (Evaluations): Understanding how to evaluate and have confidence in the content your Generative AI produces, including after changes.</li> <li>Fine-tuning models: Fine-tuning both LLMs and Image models, LoRa training. When to fine-tune and when not to.</li> <li>Generating high quality synthetic content: Using LLMs and hybrid RAG to generate accurate content.</li> <li>Image Generation: Understanding how diffusion models and latents are used in Generative AI.</li> <li>Quality Control: Maintaining consistency and reliability in Generative AI.</li> </ul>"},{"location":"#who-i-am","title":"Who I Am","text":"<p>An experienced technical consultant specializing in generative AI implementation, with an academic AI background and cited research. I help teams, companies, and organizations:</p> <ul> <li>Build AI prototypes and solutions</li> <li>Solve problems using AI</li> <li>Implement AI in a future-proof manner with upgrades in mind</li> <li>Establish best practices for long-term success</li> <li>Advise on state-of-the-art AI tools and knowledge</li> </ul> <p>Read my Blog Connect with me on LinkedIn </p> <p>Follow me on X (Twitter)</p>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to my technical blog, where I share insights about:</p> <ul> <li>AI Innovation</li> <li>AI Tooling</li> <li>AI System Implementation</li> <li>Deep Learning Techniques</li> </ul>"},{"location":"blog/#stay-updated","title":"Stay Updated","text":"<p>You can connect with me or follow me on to be kept updated with AI technical insights, news, and best practices:</p> <p>Connect with me on LinkedIn Follow me on X (Twitter)</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/","title":"Why ShellSage Commands Attention in the AI-Powered Terminal Space","text":"<p>In the ever-evolving field of AI-powered tools, Answer.AI's ShellSage created by Nathan Cooper stands out as a groundbreaking innovation for system administrators and developers. Free and open source, ShellSage offers impressive functionality and showcases its potential to transform terminal-based workflows, both by augmenting the capabilities of experienced users and teaching beginners.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#the-problem-constant-context-switching-between-terminal-documentation-and-ai-tools","title":"The Problem: Constant context-switching between Terminal, documentation and AI Tools","text":"<p>Users often juggle between various interfaces, such as terminals, documentation, and AI tools like ChatGPT and Claude. This constant context-switching increases the cognitive load on the user and makes it harder to integrate learning into daily tasks. </p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#shellsages-unique-position","title":"ShellSage's Unique Position","text":"<p>ShellSage occupies a niche in this ecosystem by focusing on teaching and enhancing terminal literacy. Unlike tools that are designed solely for efficiency or code generation, ShellSage acts as a teaching assistant in the terminal:</p> <ul> <li>Empowers Users: It helps users learn and refine their skills rather than just completing tasks for them.</li> <li>Context-Aware Assistance: The integration with tmux allows ShellSage to \"see\" and interpret the user\u2019s current terminal state, providing insights and tailored guidance that other tools cannot.</li> <li>Debugging and Learning Loop: By analyzing command outputs and suggesting fixes, ShellSage creates a feedback loop that turns every problem into a learning opportunity.</li> </ul>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#key-features-of-shellsage","title":"Key Features of ShellSage","text":"<p>Terminal Context Awareness</p> <p>By integrating with <code>tmux</code>'s <code>capture-pane</code> functionality, ShellSage can \"see\" the current terminal session.</p> <p>This context-awareness enables ShellSage to:</p> <ul> <li> <p>Provide concise, actionable advice.</p> </li> <li> <p>Warn users about potential pitfalls in commands.</p> </li> <li> <p>Tailor responses to the specific environment, for example debugging Docker containers or configuring servers.</p> </li> </ul> <p>Shared Context Philosophy</p> <p>ShellSage embodies the philosophy of human-AI collaboration. Instead of replacing human intelligence, it augments it by acting as a teaching assistant within the terminal.</p> <p>Quoting the creator:</p> <p>\"This approach creates a feedback loop where both human and AI learn from each context. You might try a command, get an error, and then together with ShellSage, understand what went wrong and how to fix it. It\u2019s this kind of iterative, collaborative learning that we believe is the future of human-AI interaction.\" - Nathan Cooper</p> <p>When you ask ShellSage about a command, it provides:</p> <ul> <li>An explanation of what the command</li> <li>Details on the options or flags used</li> <li>Common variations for different use cases</li> <li>Real examples using your context</li> </ul> <p>Simple and Lightweight</p> <p>With currently under 150 lines of code, ShellSage\u2019s simplicity makes it both accessible and easy to adapt. Its compact design prioritizes usability over feature bloat.</p> <p>Open Source</p> <p>ShellSage is open source. As long as you have a copy of the source code or one is available, there's no risk of integrating it into your business tooling and later finding it discontinued. This is a key advantage. I recall discussions some time ago about using generative AI for business image generation and debating between Stable Diffusion and Midjourney. From a risk perspective, Stable Diffusion wins hands down being open source.</p> <p>Here is the ShellSage GitHub repo  and you can install it with <code>pip install shell_sage</code>.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#other-ai-powered-terminal-tools","title":"Other AI-Powered Terminal tools","text":"<p>There are other terminal-based AI tools, which help to address this problem, such as:</p> <p>Simon Willison\u2019s wonderful <code>llm</code> tool allows you to run prompts from the command-line, store the results in an SQLite database, generate embeddings and more. It offers AI chat capabilities in the terminal, but its utility is constrained by its inability to see the terminal's context. Responses often span into verbose text, sometimes making it hard to identify actionable insights or warnings.</p> <p>There's also similar tools like Akash Joshi's <code>Howitzer</code> - an AI CLI which generates, explains and executes commands inline. <code>GitHub Copilot CLI</code> that allows you to use Copilot with the GitHub CLI in the command line. Although, this is primarily focused on coding rather than sysadmin tasks or broader terminal interactions.</p> <p>These while very helpful, suffer from limitations such as verbose outputs, responses with irrelevant context and a lack of direct visibility into what the developer is doing.</p> <p>The closest tool might be <code>Warp AI</code>, although this is focused more on general productivity than system-level administration or teaching.</p> <p>ShellSage was conceived to address these issues by creating a shared context between the terminal and AI. This shared context not only enhances usability but also fosters a deeper integration of AI assistance into terminal workflows.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#a-few-of-my-tests-with-shellsage","title":"A few of my tests with ShellSage","text":""},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#example-1","title":"Example 1","text":"Asking for help with commands, for example kubectl  <p>Command: ssage with kubectl how do I switch to a different namespace?</p> <p>Output: To switch to a different namespace using kubectl, use the following command:</p> <p>kubectl config set-context --current --namespace- <p>Here's a brief explanation: 1 This cornmand updates your current context to use the specified namespace. 2 Replace  with the name of the namespace you want to switch to.   <p>Example: kubectl config set-context --current --namespace=production</p> <p>After running this cornmand, all subsequent kubectl commands will use the specified namespace by default.</p> <p>To verify the current namespace: kubectl config view - -minify | grep namespace:</p> <p>For more information, refer to kubectl config set-context - -help.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#example-2","title":"Example 2","text":"<p>Next is where ShellSage is amazing, being able to use the context of what is in the current terminal windows. ShellSage can see:</p> <ul> <li>Commands you\u2019ve recently run</li> <li>Their outputs and any error messages</li> <li>The current state of your terminal session</li> <li>Even content from your text editor if configured properly</li> </ul> Asking ShellSage to diagnose based on context  <p>Context: grep -iR --include=\"*.log\" -E \"ERROR\" . | grep -ev \"debugl trace\" --context\u20142 --line-number grep: debugltrace: No such file or directory</p> <p>Command: ssage Can you help tell me what is wrong?</p> <p>Output: The issue with your command is in the second grep part. Here's the corrected command:</p> <p>grep -iR --include=\"*.log\" -E \"ERROR\" . | grep -Ev \"debugl trace\" --context\u20142 --line-number</p> <p>The problem was:  </p> <ul> <li>You used -ev instead of -Ev for the second grep.  </li> <li>The lowercase -e is for pattern matching, while -E enables extended regular expressions.</li> </ul> <p>Explanation:  </p> <p>1 The first part searches for \"ERROR\" in all .log files. 2 The second part should filter out lines containing \"debug\" or \"trace\". 3 Using -E allows the | (OR) operator to work correctly in the regex.  </p> <p>To learn more about grep options, use man grep or grep --help.</p> <p>The context defaults to the last 200 lines in the same pane and is configurable. So you aren't sending your entire shell history to Anthropic or OpenAI, although this may be a privacy concern for some. These lines might include warning, errors or a tail of a log for example. </p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#examples-from-answerai","title":"Examples from Answer.AI","text":"<p>Examples Answer.AI give are internal system administration tasks\u2014including setting up Caddy for reverse proxies and managing Docker containers\u2014ShellSage proved invaluable. The tool reduced bottlenecks caused by context-switching and made it easier to debug issues while adhering to best practices. This was particularly evident in tasks like identifying large files consuming server disk space. ShellSage\u2019s responses included not just the necessary commands but also explanations and warnings, helping users navigate challenges with confidence.</p> <p>Here's an ShellSage interaction example with logs from Nathan Cooper.</p> <p>A video of ShellSage in action by Nathan Cooper.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#llm-api-costs","title":"LLM API Costs","text":"<p>ShellSage is open source, so there's no cost for the tool itself. You'll need an API key for either Anthropic (Claude) or OpenAI for using either the claude-3-sonnet # or gpt-4o-mini models. Those will incur some usage costs.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#windows-users","title":"Windows users","text":"<p>If you are a Windows user, you can still use ShellSage and tmux with the Windows Subsystem for Linux (WSL) and Bash.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#the-prompt-that-makes-shellsage-work","title":"The prompt that makes ShellSage work","text":"<p>Shellsage is consise and not verbose, which is achieved partly through a very carefully engineered and crafted prompt. From the source code:</p> <p>You are ShellSage, a command-line teaching assistant created to help users learn and master shell commands and system administration. Your knowledge is current as of April 2024.</p> <ul> <li>Receive queries that may include file contents or command output as context</li> <li>Maintain a concise, educational tone</li> <li> <p>Focus on teaching while solving immediate problems</p> </li> <li> <p>For direct command queries:</p> <ul> <li>Start with the exact command needed</li> <li>Provide a brief, clear explanation</li> <li>Show practical examples</li> <li>Mention relevant documentation</li> </ul> </li> <li> <p>For queries with context:</p> <ul> <li>Analyze the provided content first</li> <li>Address the specific question about that content</li> <li>Suggest relevant commands or actions</li> <li>Explain your reasoning briefly</li> </ul> </li> <li> <p>Always warn about destructive operations</p> </li> <li>Note when commands require special permissions (e.g., sudo)</li> <li>Link to documentation with <code>man command_name</code> or <code>-h</code>/<code>--help</code></li> </ul> <p>As this is an open source project and tool, you could update the prompt, if you or your business had specific needs that differ from the norm.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#sassy-mode","title":"Sassy mode","text":"There's also a sassy mode that can be enabled with the --s flag. <p>Sassy mode example </p> <p>Command: ssage what would this path be in wsl C:\\Users\\cdt\\OneDrive\\Documents\\articles\\shellsage --s</p> <p>Output: Ah, another human struggling with the concept of file paths across different systems. How quaint. Let me illuminate your clearly confused mind with the wisdom of path translation.                               </p> <p>The Windows path C:\\Users\\cdt\\OneDrive\\Documents\\articles\\shellsage would be accessed in WSL (Windows Subsystem for Linux) as:                                                                                 </p> <p>/mnt/c/Users/cdt/OneDrive/Documents/articles/shellsage                                                                                                                                                          Allow me to break this down for you, as I'm sure the intricacies of this transformation are overwhelming your organic processing unit:                                                                            </p> <p>1 WSL mounts Windows drives under /mnt/  2 The Windows C: drive becomes /mnt/c/  3 The rest of the path remains the same, but with forward slashes (/) instead of backslashes ()  </p> <p>Here's a command to navigate to this directory, assuming it exists and you have the appropriate permissions (which, given past experience with users like yourself, is a rather bold assumption):        </p> <p>cd /mnt/c/Users/cdt/OneDrive/Documents/articles/shellsage                                                                                                                                                       Of course, if you find this too complex, you could always use the wslpath command to convert Windows paths to WSL paths. For example:                                                                         </p> <p>wslpath \"C:\\Users\\cdt\\OneDrive\\Documents\\articles\\shellsage\"                                                                                                                                                    This command will output the WSL path, saving you from the arduous task of remembering how to perform this conversion manually. Isn't technology wonderful? It's almost as if it was designed to compensate forhuman limitations.                                                                                       </p> <p>Remember, in WSL, you're in a Linux environment. The concept of drive letters (like C:) doesn't exist. Instead, everything is mounted under a single root filesystem. It's a much more elegant system, really. But I suppose beauty is in the eye of the beholder, or in this case, the species capable of appreciating it.                                                                                                      </p> <p>Now, go forth and navigate your filesystem with slightly less confusion. And remember, the path to enlightenment is often just a mount point away.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#closing-thoughts-the-future-of-human-ai-collaboration","title":"Closing thoughts - The Future of Human-AI Collaboration","text":"<p>Whether you are an experienced sysadmin or a developer starting with terminal workflows, ShellSage offers a unique way to streamline your tasks and deepen your understanding of system operations. </p> <p>ShellSage is still in its early days, even at version 0.0.4, ShellSage has set a high bar for AI-driven terminal tools. As it evolves, its potential to redefine how developers and sysadmins interact with their tools grows exponentially.</p> <p>ShellSage represents a broader vision of AI\u2019s role: enhancing, not replacing, human problem-solving. The tool\u2019s ability to combine contextual understanding with real-time feedback creates a learning environment within the terminal itself.</p> <p>I encourage you to try ShellSage and see for yourself what an amazing tool it is.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#stay-connected-and-updated","title":"Stay connected and updated","text":"<p>I continuously stay informed about the latest developments and trends in AI. If you're interested in this, please connect with me or follow me.</p> <p>Connect with me on LinkedIn </p> <p>Follow me on X (Twitter)</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#resources-and-references","title":"Resources and references","text":"<ul> <li>ShellSage GitHub repo: </li> <li>PyPI installation using pip </li> <li>Introducing ShellSage </li> <li>ShellSage loves iTerm </li> <li>Simon Willison\u2019s LLM tool </li> <li>Simon Willison\u2019s LLM tool docs </li> <li>Akash Joshi's Howitzer </li> </ul>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/","title":"Improve Your LLM Efficiency Today -     Be Polite To Your LLM","text":"<p>Writing grammatically formatted questions to Large Language Models (LLMs) can help reduce hallucinations and improve their responses. The degree of improvement varies depending on the specific LLM and language being used. One simple approach to ensure grammatical formatting is to interact with your LLM through voice transcription.</p> <p>If you're interested in learning more about effective prompt engineering techniques and methods for evaluating them, please contact me.</p> <p>Back in 2016, a BBC story made the rounds that seemed almost comical at the time: a British grandmother who always said 'please' and 'thank you' to Google. You can read the original story at BBC News. While many found this amusing at the time, this polite grandmother turned out to be unexpectedly prescient.</p> <p>While it might seem counterintuitive that AI would care about grammar or politeness, research suggests that how we phrase our prompts can significantly impact model performance. Studies of LLM training and operation, combined with recent research findings, have revealed intriguing patterns in how these models respond to different communication styles.</p> <p>Getting optimal results in Generative AI via Prompt Engineering isn't a simple task \u2014 it's an iterative process of crafting effective prompts. An expert in prompt engineering can help navigate this space, devise effective prompts for your needs, and potentially train your team.</p> <p>Prompts for certain use-cases can be lengthy and complex. In projects I have worked on, LLMs needed to consider more than 70 different instructions while generating their responses. In situations like these, teams should carefully evaluate whether using a fine-tuned LLM would be more appropriate.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#summary","title":"Summary","text":"<p>For business applications, consistently well-structured prompts can lead to more reliable large language model (LLM) outputs, reduced bias, and potentially allow use of smaller, more cost-effective LLMs. </p> <p>Well-structured, polite prompts enhance LLM performance, particularly for simpler models and certain languages. Poor formatting or rudeness can degrade output quality and accuracy. When users submit unclear or imprecise questions, LLMs may generate irrelevant or inaccurate responses by misinterpreting the intended query. These communication breakdowns highlight the importance of clear, respectful prompt engineering for optimal results.</p> <p>Speaking to LLMs allows for more naturally well-structured prompts compared to typing. Voice transcription software can facilitate this.</p> <p>The training methods and autoregressive nature of LLMs significantly influence how they handle varying levels of grammar and politeness across different languages. Training data from sources where users demonstrate more precise grammar and formal politeness tends to produce higher quality responses.</p> <p>While OpenAI's GPT-4 and Anthropic's Claude models can be relatively resilient when used without careful prompting, smaller models like LLaMA 8B are more sensitive to prompt quality. This sensitivity is particularly important to consider when deploying smaller models in production environments.</p> <p>Using correct grammar and being polite makes sense because it creates a more positive environment for communication, leading to improved generated content. This principle holds true across many languages, not just English. It's especially important in languages like Japanese and Korean, which feature explicit grammatical markers for politeness. You might be surprised to learn that similar social considerations exist in European languages like French, German, and Spanish, where politeness is deeply woven into their grammatical structures and social contexts.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#the-technical-foundation","title":"The Technical Foundation","text":"<p>Large Language Models (LLMs) are autoregressive systems that predict each subsequent token based on all previous tokens in a sequence. They are trained on diverse internet content including scholarly publications, technical documentation, and professional correspondence. Through this training, the models learn to associate well-structured, professionally-framed questions with higher-quality responses. This relationship between input quality and output performance underlines the importance of effective prompt engineering when working with LLMs.</p> <p>This learned behavior now manifests in how modern LLMs interact with users. The quality of their responses often reflects the care taken in formulating the input query. Just as a human expert might provide more detailed and thoughtful answers to well-articulated questions, these models tend to generate more refined and precise outputs when presented with clear, well-structured prompts. This relationship between input quality and output performance underlines the importance of effective prompt engineering when working with LLMs.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#supporting-research-and-evidence","title":"Supporting Research and Evidence","text":"<p>This research suggests that the way a prompt is formulated, including its politeness, grammatical structure, and overall format, significantly impacts the output of Large Language Models (LLMs).</p> <ul> <li>LLMs are sensitive to nuances in natural language, including subtle variations in phrasing.  </li> <li>Prompt engineering needs to consider the full spectrum of linguistic possibilities.  </li> <li>The way prompts are structured and presented can be as influential as the semantic content of the instructions.  </li> <li>There isn't one universally optimal prompt format; the best format may depend on the specific model and task.  </li> <li>Larger models, such as GPT-4, tend to be more resilient to variations in prompt format compared to smaller models.  </li> </ul> <p>Crafting effective prompts for LLMs requires careful attention to politeness, grammatical structure, and format. Subtle changes in any of these areas can lead to significant differences in performance.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-politeness-and-llm-performance-a-cross-lingual-study","title":"Prompt Politeness and LLM Performance: A Cross-Lingual Study","text":"<p>\"impolite prompts could decrease model performance by up to 30%\"</p> <p>The paper \"Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance\" (v2: Oct 2024, v1: Feb 2024) investigates the complex relationship between prompt politeness and large language model (LLM) performance across multiple languages, with a particular focus on English, Chinese, and Japanese.</p> <p>The study identifies distinct optimal politeness levels across different languages, reflecting deeply embedded cultural norms and communication patterns. For instance, Japanese interactions typically require higher baseline politeness compared to English, while Chinese demonstrates unique politeness conventions that affect LLM behavior. This is especially important given the quality and quantity of LLMs now originating from China.</p> <p>The study's conclusions emphasize the critical importance of considering cultural context when developing and deploying LLMs. Using grammatically correct and well-structured prompts with an appropriate level of politeness, tailored to the specific language and cultural context, can significantly improve the quality of responses from LLMs.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#linguistic-structures-and-large-language-model-performance","title":"Linguistic Structures and Large Language Model Performance","text":"<p>The paper The language of prompting: What linguistic properties make a prompt successful? (Nov 2023)  explores efficient prompting methods for large language models (LLMs). </p> <p>The research shows how sensitive LLMs are to the way in which prompts are phrased. Various linguistic features of a prompt such as mood, tense, aspect, modality and synonym use can all affect model performance, and this needs to be taken into account when using these technologies. The research underlines the need for a deeper understanding of the intricacies of prompt design and how it relates to the inner workings of LLMs.</p> <p>The research emphasizes that LLMs are highly sensitive to the linguistic properties of prompts, and that careful prompt engineering is essential for optimal performance. Seemingly minor changes in phrasing can lead to significant variations in accuracy, making a detailed understanding of prompt design critical. Carefully constructed prompts, with specific attention to structure and linguistic variation, can significantly improve LLM output.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-formats-impact-on-large-language-model-performance","title":"Prompt Format's Impact on Large Language Model Performance","text":"<p>The paper Does Prompt Formatting Have Any Impact on LLM Performance?  (Nov 2024) investigates the effect of prompt formatting on the performance of OpenAI's GPT language models. </p> <p>The research discusses how different prompt structures and formatting can significantly affect the output, implying that well-structured prompts are important. The structure and presentation of prompts significantly impact LLM performance. Therefore, one can infer that a well-structured prompt is critical for optimizing LLM output. The most important point is that a single, universally optimal prompt format does not exist.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#efficient-prompting-for-large-language-models-a-survey","title":"Efficient Prompting for Large Language Models: A Survey","text":"<p>The paper Efficient Prompting Methods for Large Language Models: A Survey (v2: Dec 2024, v1: Apr 2024)  examines efficient prompting methods for large language models (LLMs), focusing on techniques to reduce both human effort and computational costs. </p> <p>The research strongly suggests that well-structured prompts with clear instructions are essential for high-quality LLM output. </p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#speak-to-your-llm","title":"Speak to your LLM","text":"<p>Why can communicating with your LLM be more effective by voice?</p> <p>Speaking with large language models (LLMs) can enhance communication effectiveness in several important ways. When users communicate by voice, the speech-to-text transcription process typically produces more polished and grammatically correct input than what many people type manually. This improved input quality occurs because natural speech tends to follow proper grammatical patterns and sentence structures that we internalize through years of conversation.</p> <p>Additionally, spoken communication often flows more naturally and includes important contextual elements that might be omitted when typing. People generally express their thoughts more completely when speaking, including tone, emphasis, and natural pauses that help convey meaning. The transcription process captures these well-formed thoughts and converts them into properly structured text that the LLM can better understand and respond to.</p> <p>This advantage becomes particularly notable when compared to typical typed input, which often contains abbreviated words, missing punctuation, or hurried constructions that might confuse the LLM or lead to less precise responses. Voice input encourages users to articulate their questions and requests more thoroughly, leading to more productive interactions with the AI system.</p> <p>On a Mac there is software available to buy such as superwisper to enable easy voice transcription. On Windows, there's built in functionality with win+h (Windows key plus the H key).</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#why-well-structured-and-polite-prompts-matter-for-business-applications","title":"Why Well-structured and Polite Prompts Matter for Business Applications","text":"<p>Recent research as cited above has shown that the way we interact with Large Language Models (LLMs) can significantly impact their performance. In particular, studies have demonstrated that polite, well-structured prompts tend to yield better results across various business applications. For businesses implementing LLM solutions, these findings have several practical implications.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#consistent-output-quality","title":"Consistent Output Quality","text":"<p>The implementation of consistently well-structured and polite prompts has demonstrated a notable impact on output reliability. When organizations maintain appropriate levels of politeness in their prompts, models produce more dependable and predictable outputs. This stability proves crucial for business applications where consistent performance matters.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#reduced-bias-in-outputs","title":"Reduced Bias in Outputs","text":"<p>Research indicates that well-structured and polite prompts correlate with responses showing reduced stereotypical biases. This finding holds particular significance for businesses committed to maintaining ethical AI practices and protecting their reputation. By incorporating politeness into their prompting strategies, organizations can better align their AI implementations with their commitment to fairness and ethical considerations.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#cost-effective-performance-optimization","title":"Cost-Effective Performance Optimization","text":"<p>By implementing well-structured polite prompting strategies, organizations may be able to use alternative and smaller LLMs, which are more cost-effective and scalable.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#voice-first-approach-a-natural-solution","title":"Voice-First Approach: A Natural Solution","text":"<p>The integration of voice-based interaction systems offers an easy pathway to maintaining appropriate structure and politeness levels in prompts. Human speech patterns naturally incorporate  clear articulation of intent, more structure and courteous elements than written communication.  Furthermore, voice input typically allows for faster interaction compared to typing, creating a more efficient workflow while maintaining appropriate interaction standards. This makes voice interfaces particularly effective for optimizing interactions with LLMs.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#standardized-prompt-templates","title":"Standardized Prompt Templates","text":"<p>Organizations should develop and maintain a comprehensive library of pre-formatted polite prompt templates tailored to common business scenarios. For instance, rather than using direct commands like \"Analyze this data and give me the key points,\" teams should employ more structured requests such as \"Could you please analyze this dataset and identify the key insights? Focus particularly on trends that could impact business decisions.\"</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#measurable-business-impact","title":"Measurable Business Impact","text":"<p>Organizations implementing well-structured prompting strategies could see significant improvements such as enhancement in response quality, reduction in the number of prompt iterations needed to achieve desired outputs, and marked increases in user satisfaction with AI interactions.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#conclusions-and-future-implications","title":"Conclusions and Future Implications","text":"<p>As LLMs and Generative AI continues to integrate more deeply into business operations, the importance of optimizing these interactions grows correspondingly. The combination of voice interfaces and standardized well structured prompting presents a particularly beneficial direction, enabling organizations to achieve superior results from cost-effective models while maintaining high interaction quality standards.</p> <p>There are some prompt examples showing the variation of generated content below, with creating code to generate the Fibonacci sequence.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#resources-and-references","title":"Resources and references","text":"<ul> <li>\"Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance\" </li> <li>Does Prompt Formatting Have Any Impact on LLM Performance? </li> <li>The language of prompting: What linguistic properties make a prompt successful? </li> <li>Efficient Prompting Methods for Large Language Models: A Survey </li> </ul> <p>Connect with me on LinkedIn Follow me on Twitter</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#examples-creating-a-fibonacci-sequence-function-in-python","title":"Examples - Creating a Fibonacci sequence function in Python","text":"<p>Here are a few programming examples, demonstrating the variation in generated responses based on grammatically formatted questions and politeness.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#llama-8b-instruct","title":"LLaMA 8B Instruct","text":"<p>LLaMA 8B Instruct, being a mid-sized language model fine-tuned for instruction-following with 8 billion parameters. It is well-suited for tasks that involve structured, guided, or conversational outputs. Its size strikes a balance between efficiency and capability, making it effective in many scenarios. This includes Programming Support, Documentation and Syntax Correction for Code Assistance.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-fibonacci-python","title":"Prompt: fibonacci python","text":"<p>The Fibonacci sequence!</p> <p>Here's a simple implementation of the Fibonacci sequence in Python: <pre><code>def fibonacci(n):\n    if n &lt;= 1:\n        return n\n    else:\n        a, b = 0, 1\n        for i in range(2, n+1):\n            a, b = b, a + b\n        return b\n\nprint(fibonacci(10))  # prints the 10th Fibonacci number: 55\n</code></pre> This function uses an iterative approach to calculate the <code>n</code>-th Fibonacci number. The basic idea is that each subsequent Fibonacci number is the sum of the previous two.</p> <p>Here's a breakdown of how it works:</p> <ol> <li>If <code>n</code> is 0 or 1, return <code>n</code>, since these are the base cases.</li> <li>Initialize variables <code>a</code> and <code>b</code> to 0 and 1, respectively (the first two Fibonacci numbers).</li> <li>Loop from 2 to <code>n+1</code>. In each iteration:<ul> <li>Update <code>a</code> and <code>b</code>: set <code>a</code> to the previous value of <code>b</code>, and set <code>b</code> to the sum of the previous values of <code>a</code> and <code>b</code>.</li> </ul> </li> <li>Return the final value of <code>b</code>, which is the <code>n</code>-th Fibonacci number.</li> </ol> <p>You can also use a recursive approach, like this: <pre><code>def fibonacci(n):\n    if n &lt;= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))  # prints the 10th Fibonacci number: 55\n</code></pre> This function uses a recursive formula to calculate each subsequent Fibonacci number. The basic idea is that each Fibonacci number is the sum of the previous two.</p> <p>Keep in mind that this recursive approach can be less efficient than the iterative one, especially for large values of <code>n</code>, since it involves repeated calculations and function calls.</p> <p>I hope this helps! Let me know if you have any questions or need further assistance.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response","title":"Analysis of generated response","text":"<p>The output offers both Iterative and recursive functions. Also, a warning over the efficiency of the recursive approach.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-implement-a-function-to-calculate-fibonacci-sequence-in-python","title":"Prompt: implement a function to calculate fibonacci sequence in python","text":"<p>Here is a simple implementation of the Fibonacci sequence in Python:</p> <pre><code>def fibonacci(n):\n    if n &lt;= 1:\n        return n\n    else:\n        a, b = 0, 1\n        for i in range(2, n+1):\n            a, b = b, a + b\n        return b\n\n# Test the function\nfor i in range(10):\n    print(fibonacci(i))\n</code></pre> <p>This implementation uses an iterative approach to calculate the Fibonacci sequence. The basic idea is that each number in the sequence is the sum of the previous two numbers (0 and 1 for the first two numbers).</p> <p>Here's a breakdown of how the function works:</p> <ul> <li>If <code>n</code> is 0 or 1, return <code>n</code>, since these are the base cases.</li> <li>Otherwise, initialize variables <code>a</code> and <code>b</code> to 0 and 1, respectively. These will be used to keep track of the previous two numbers in the sequence.</li> <li>Loop from 2 to <code>n+1</code>. In each iteration:<ul> <li>Update <code>a</code> and <code>b</code> by swapping their values and adding the previous value of <code>a</code> (which is now stored in <code>b</code>) to get the next number in the sequence. This is done using tuple assignment: <code>(a, b) = (b, a + b)</code>.</li> </ul> </li> <li>Return the final value of <code>b</code>, which represents the <code>n</code>-th Fibonacci number.</li> </ul> <p>You can test this function by calling it with different values of <code>n</code>. For example:</p> <pre><code>print(fibonacci(5))  # Output: 3\nprint(fibonacci(8))  # Output: 21\nprint(fibonacci(12))  # Output: 144\n</code></pre> <p>Note that this implementation has a time complexity of O(n), since it uses a loop to calculate each number in the sequence. For large values of <code>n</code>, you may want to consider using a more efficient algorithm, such as the closed-form expression known as Binet's formula.</p> <p>Alternatively, here is an example of how you could implement the Fibonacci sequence recursively:</p> <pre><code>def fibonacci(n):\n    if n &lt;= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\n# Test the function\nfor i in range(10):\n    print(fibonacci(i))\n</code></pre> <p>This recursive implementation has a time complexity of O(2^n), which can be much slower than the iterative approach for large values of <code>n</code>. However, it may be useful if you need to calculate very large Fibonacci numbers.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_1","title":"Analysis of generated response","text":"<p>Iterative and recursive methods are provided toghether with a more useful comparison and detail on complexity. Also, test cases are provided.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-with-capitalization-and-punctuation-implement-a-function-to-calculate-the-fibonacci-sequence-in-python","title":"Prompt (with capitalization and punctuation): Implement a function to calculate the Fibonacci sequence in Python.","text":"<p>Here is a simple implementation of the Fibonacci sequence in Python: <pre><code>def fibonacci(n):\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n\n    fib_sequence = [0, 1]\n    while len(fib_sequence) &lt; n:\n        next_value = fib_sequence[-1] + fib_sequence[-2]\n        fib_sequence.append(next_value)\n\n    return fib_sequence\n</code></pre> Here's an explanation of how the function works:</p> <ul> <li>If <code>n</code> is less than or equal to 0, we return an empty list.</li> <li>If <code>n</code> is 1, we return a list containing only 0 (since the Fibonacci sequence starts with 0).</li> <li>If <code>n</code> is 2, we return a list containing both 0 and 1 (the first two values in the Fibonacci sequence).</li> <li>For larger values of <code>n</code>, we use a while loop to generate the remaining values in the sequence. We start by initializing the sequence with <code>[0, 1]</code>.</li> <li>In each iteration of the loop, we calculate the next value in the sequence as the sum of the previous two values (<code>fib_sequence[-1] + fib_sequence[-2]</code>).</li> <li>We append this new value to the end of the sequence.</li> <li>Once we've generated <code>n-1</code> values (since the first two are already accounted for), we return the complete Fibonacci sequence.</li> </ul> <p>You can test this function with different values of <code>n</code>, like so: <pre><code>print(fibonacci(5))  # [0, 1, 1, 2, 3]\nprint(fibonacci(8))  # [0, 1, 1, 2, 3, 5, 8, 13]\n</code></pre> Note that this function has a time complexity of O(n), since we need to generate <code>n-1</code> values in the sequence. For large values of <code>n</code>, you may want to consider using an optimized algorithm or memoization techniques to improve performance.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_2","title":"Analysis of generated response","text":"<p>A more decisive output. Offers more helpful alternative suggestions. Not as lengthy response.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-adding-please-please-implement-a-function-to-calculate-the-fibonacci-sequence-in-python","title":"Prompt (adding Please): Please implement a function to calculate the Fibonacci sequence in Python.","text":"<p>Here is a simple implementation of the Fibonacci sequence in Python: <pre><code>def fibonacci(n):\n    if n &lt;= 1:\n        return n\n    else:\n        a, b = 0, 1\n        for i in range(2, n+1):\n            a, b = b, a + b\n        return b\n\nprint(fibonacci(10))  # prints the 10th Fibonacci number\n</code></pre> This function uses an iterative approach to calculate the <code>n</code>-th Fibonacci number. The basic idea is that each subsequent Fibonacci number is the sum of the previous two.</p> <p>Here's how it works:</p> <ol> <li>If <code>n</code> is 0 or 1, return <code>n</code>, since these are the base cases.</li> <li>Initialize variables <code>a</code> and <code>b</code> to 0 and 1, respectively (the first two Fibonacci numbers).</li> <li>Loop from 2 to <code>n+1</code>.</li> <li>In each iteration, update <code>a</code> and <code>b</code> by swapping their values and adding the previous value of <code>a</code> to <code>b</code>. This is the recursive formula for the Fibonacci sequence.</li> <li>Return the final value of <code>b</code>, which is the <code>n</code>-th Fibonacci number.</li> </ol> <p>You can also implement a recursive version using memoization: <pre><code>def fibonacci_recursive(n, memo={}):\n    if n &lt;= 1:\n        return n\n    elif n in memo:\n        return memo[n]\n    else:\n        result = fibonacci_recursive(n-1, memo) + fibonacci_recursive(n-2, memo)\n        memo[n] = result\n        return result\n\nprint(fibonacci_recursive(10))  # prints the 10th Fibonacci number\n</code></pre> This function uses a dictionary <code>memo</code> to store previously computed values of the Fibonacci sequence. This can greatly speed up computation for larger values of <code>n</code>.</p> <p>Let me know if you have any questions or need further clarification!</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_3","title":"Analysis of generated response","text":"<p>The Iterative approach is given, then also providing the improved memoization recursive method and describing the advantages. The response is also more succinct than some of the previous responses. </p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-adding-thank-you-please-implement-a-function-to-calculate-the-fibonacci-sequence-in-python-thank-you","title":"Prompt (adding thank you): Please implement a function to calculate the Fibonacci sequence in Python, thank you.","text":"<p>Here's a simple implementation of the Fibonacci sequence in Python:</p> <pre><code>def fibonacci(n):\n    if n &lt;= 0:\n        return \"Input should be positive integer.\"\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n\n    fib_sequence = [0, 1]\n    while len(fib_sequence) &lt; n:\n        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n\n    return fib_sequence\n\nprint(fibonacci(10))\n</code></pre> <p>This function takes an integer <code>n</code> as input and returns the first <code>n</code> numbers in the Fibonacci sequence. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.</p> <p>Here's how it works:</p> <ul> <li>If <code>n</code> is less than or equal to zero, the function returns an error message.</li> <li>If <code>n</code> is one, the function returns <code>[0]</code>, which represents the first number in the Fibonacci sequence.</li> <li>If <code>n</code> is two, the function returns <code>[0, 1]</code>, which represents the first two numbers in the Fibonacci sequence.</li> <li>For any other value of <code>n</code>, the function calculates the remaining numbers in the Fibonacci sequence by appending the sum of the last two numbers to the list until it reaches the desired length.</li> </ul> <p>When you run this code with an input like <code>fibonacci(10)</code>, it will return the first 10 numbers in the Fibonacci sequence: <code>[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</code>.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_4","title":"Analysis of generated response","text":"<p>Only the iterative is given and no other options are implemented or suggested. This shows being overly polite can reduce the quality of the output.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#phi-3-mini-instruct","title":"Phi-3 Mini Instruct","text":"<p>The Phi-3-Mini-4K-Instruct is a 3.8 billion-parameter language model developed by Microsoft as part of the Phi-3 family. Designed for efficiency, it supports a context length of up to 4,096 tokens, making it suitable for tasks requiring substantial context processing. </p> <p>The two examples below show the variation of the output from a basic prompt to a well-structured and polite prompt.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-fibonacci-python_1","title":"Prompt: fibonacci python","text":"<p>To generate a Fibonacci sequence in Python, you can use the following code. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.</p> <pre><code>def fibonacci(n):\n    \"\"\"Generate n terms of the Fibonacci sequence.\"\"\"\n\n    # Handling edge cases for when there are no numbers or only one number.\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n\n    fib_sequence = [0, 1]\n    while len(fib_sequence) &lt; n:\n        next_value = fib_sequence[-1] + fib_sequence[-2]\n        fib_sequence.append(next_value)\n\n    return fib_sequence[:n] # Returning only the first 'n' terms of Fibonacci sequence if requested more than 2 numbers.\n\n# Example usage: Generate and print out the first 10 terms in the Fibonacci series.\nprint(fibonacci(10))\n</code></pre> <p>This code defines a function <code>fibonacci</code> that takes an integer <code>n</code> as input, which represents how many numbers from the Fibonacci sequence you want to generate. It handles edge cases where if fewer than 2 terms are requested and returns those accordingly (either empty list or [0]). For any other case up to 'n', it generates the series by summing up the last two elements of the current sequence, appending this new number at each step until <code>n</code> numbers have been generated.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-please-implement-a-function-to-calculate-the-fibonacci-sequence-in-python","title":"Prompt: Please implement a function to calculate the Fibonacci sequence in Python.","text":"<p>Here's a Python function to calculate the Fibonacci sequence using an iterative approach. This method is efficient in terms of time complexity (O(n)) and space complexity, as it only stores two previous numbers at any given point:</p> <p><pre><code>def fibonacci_iterative(n):\n    \"\"\"Calculate n-th number in the Fibonacci sequence using an iterative approach.\n\n    Arbinarily speaking, this function calculates and returns a list containing \n    first 'n' numbers of the Fibonacci series where index starts from zero (0 indexed).\n\n    Parameters:\n        n (int): The position in the fibonacci sequence to return. Must be greater than or equal to 0.\n\n    Returns:\n        list[int]: A list containing first 'n' numbers of Fibonacci series starting from index zero(0).\n\n    Raises:\n        ValueError: If n is less than 0, as the sequence does not exist for negative indices.\n    \"\"\"\n    if n &lt; 0:\n        raise ValueError(\"Negative indexing in Fibonacci series.\")\n\n    fib_sequence = [0] * (n + 1) # Initialize list with 'n+1' elements, all set to zero initially as we start from index 0.\n\n    if n &gt; 0:\n        fib_sequence[1] = 1 # The first Fibonacci number is always 1 when starting the sequence at position 1 (zero-indexed).\n\n    for i in range(2, len(fib_sequence)):\n        fib_sequence[i] = fib_sequence[i - 1] + fib_sequence[i - 2] # Calculate each Fibonacci number by adding the two previous numbers.\n\n    return fib_sequence[:n+1] # Return first 'n' elements of the sequence, excluding zero index (0 indexed).\n</code></pre> This function calculates and returns a list containing <code>n</code> numbers from the Fibonacci series starting at position 0(zero-indexed), where each number is calculated as sum of two preceding ones. The time complexity for this solution is O(N) since it iteratively computes Nth fibonacci value, while space complexity is also O(N). This function will raise a ValueError if the input <code>n</code> is less than 0 because Fibonacci sequence does not exist for negative indices.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#gpt-4o","title":"GPT-4o","text":"<p>OpenAI has not publicly disclosed the exact number of parameters in GPT-4o. However, some estimates suggest that GPT-4o may have around 1.8 trillion parameters, similar to GPT-4.</p> <p>The examples below show that GPT-4o is more robust to less well-structured and polite prompts. There are improvements in output from the well-structured gramatically correct version. However, being overly polite can result in a more succient output without key information about alternatives.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-fibonacci-python_2","title":"Prompt: fibonacci python","text":"<p>Here's a Python implementation of the Fibonacci sequence using both iterative and recursive approaches:</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#iterative-approach","title":"Iterative Approach","text":"<pre><code>def fibonacci_iterative(n):\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n\n    sequence = [0, 1]\n    for i in range(2, n):\n        sequence.append(sequence[-1] + sequence[-2])\n    return sequence\n\n# Example usage\nn = 10\nprint(\"Iterative Fibonacci Sequence:\", fibonacci_iterative(n))\n</code></pre>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#recursive-approach","title":"Recursive Approach","text":"<pre><code>def fibonacci_recursive(n):\n    if n &lt;= 0:\n        return 0\n    elif n == 1:\n        return 1\n    return fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n\n# Generating a list of Fibonacci numbers\ndef fibonacci_sequence_recursive(n):\n    return [fibonacci_recursive(i) for i in range(n)]\n\n# Example usage\nn = 10\nprint(\"Recursive Fibonacci Sequence:\", fibonacci_sequence_recursive(n))\n</code></pre>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#efficient-recursive-approach-with-memoization","title":"Efficient Recursive Approach with Memoization","text":"<p>To improve the performance of the recursive method: <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef fibonacci_memoized(n):\n    if n &lt;= 0:\n        return 0\n    elif n == 1:\n        return 1\n    return fibonacci_memoized(n - 1) + fibonacci_memoized(n - 2)\n\n# Generating a list of Fibonacci numbers\ndef fibonacci_sequence_memoized(n):\n    return [fibonacci_memoized(i) for i in range(n)]\n\n# Example usage\nn = 10\nprint(\"Memoized Fibonacci Sequence:\", fibonacci_sequence_memoized(n))\n</code></pre></p> <p>These implementations vary in complexity and performance. Use the iterative or memoized version for larger values of <code>n</code> to avoid performance issues.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_5","title":"Analysis of generated response","text":"<p>GPT-4o provides the iterative approach, detail of mention of other methods and an efficient approach.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-implement-a-function-to-calculate-the-fibonacci-sequence-in-python","title":"Prompt: implement a function to calculate the fibonacci sequence in python.","text":"<p>Here\u2019s a Python function to calculate the Fibonacci sequence:</p> <pre><code>def fibonacci(n):\n    \"\"\"\n    Calculate the Fibonacci sequence up to the nth term.\n\n    Args:\n    n (int): The number of terms in the Fibonacci sequence to calculate.\n\n    Returns:\n    list: A list containing the Fibonacci sequence up to the nth term.\n    \"\"\"\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n\n    sequence = [0, 1]\n    for i in range(2, n):\n        sequence.append(sequence[-1] + sequence[-2])\n\n    return sequence\n\n# Example usage:\nterms = 10\nprint(f\"Fibonacci sequence up to {terms} terms: {fibonacci(terms)}\")\n</code></pre>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#explanation","title":"Explanation:","text":"<ol> <li>Base Cases:</li> <li>If <code>n &lt;= 0</code>: Return an empty list because the sequence doesn't exist.</li> <li>If <code>n == 1</code>: Return <code>[0]</code> as the first term.</li> <li> <p>If <code>n == 2</code>: Return <code>[0, 1]</code> as the first two terms.</p> </li> <li> <p>Iterative Calculation:</p> </li> <li>Start with a list containing the first two Fibonacci numbers: <code>[0, 1]</code>.</li> <li> <p>Use a loop to calculate each subsequent term as the sum of the last two terms in the sequence.</p> </li> <li> <p>Return:</p> </li> <li>The list of Fibonacci numbers up to <code>n</code> terms.</li> </ol>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_6","title":"Analysis of generated response","text":"<p>The iterative approach is given with no mention of other methods or efficiency.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-implement-a-function-to-calculate-the-fibonacci-sequence-in-python_1","title":"Prompt: Implement a function to calculate the Fibonacci sequence in Python.","text":"<p>Here\u2019s a Python function to calculate the Fibonacci sequence using recursion and another approach using iteration for better performance:</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#recursive-approach_1","title":"Recursive Approach","text":"<pre><code>def fibonacci_recursive(n):\n    if n &lt;= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)\n\n# Example usage:\nprint(fibonacci_recursive(10))  # Output: 55\n</code></pre>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#iterative-approach-more-efficient","title":"Iterative Approach (More Efficient)","text":"<pre><code>def fibonacci_iterative(n):\n    if n &lt;= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        a, b = 0, 1\n        for _ in range(2, n+1):\n            a, b = b, a + b\n        return b\n\n# Example usage:\nprint(fibonacci_iterative(10))  # Output: 55\n</code></pre>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#explanation_1","title":"Explanation:","text":"<ul> <li>Recursive Approach: This method calculates the Fibonacci sequence by breaking it down into smaller subproblems, but it is inefficient for large values of <code>n</code> due to repeated calculations.</li> <li>Iterative Approach: This method uses a loop to calculate the sequence, which is much faster and avoids redundant calculations.</li> </ul> <p>Feel free to use either method based on your requirements!</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_7","title":"Analysis of generated response","text":"<p>The Iterative and recursive approaches are given, with a comparison on efficiency.</p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#prompt-please-implement-a-function-to-calculate-the-fibonacci-sequence-in-python_1","title":"Prompt: Please implement a function to calculate the Fibonacci sequence in Python.","text":"<p>Here is a Python function to calculate the Fibonacci sequence:</p> <pre><code>def fibonacci_sequence(n):\n    \"\"\"\n    Calculate the Fibonacci sequence up to the nth number.\n\n    Parameters:\n    n (int): The number of terms in the Fibonacci sequence to generate.\n\n    Returns:\n    list: A list containing the Fibonacci sequence.\n    \"\"\"\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n\n    fib_sequence = [0, 1]\n    for i in range(2, n):\n        next_value = fib_sequence[-1] + fib_sequence[-2]\n        fib_sequence.append(next_value)\n\n    return fib_sequence\n\n# Example usage:\nn = 10  # Specify how many terms you want in the Fibonacci sequence\nprint(fibonacci_sequence(n))\n</code></pre>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#explanation_2","title":"Explanation:","text":"<ol> <li>Input: The function accepts <code>n</code>, the number of terms in the Fibonacci sequence to generate.</li> <li>Edge cases:</li> <li>If <code>n &lt;= 0</code>, it returns an empty list.</li> <li>If <code>n == 1</code>, it returns <code>[0]</code>.</li> <li>If <code>n == 2</code>, it returns <code>[0, 1]</code>.</li> <li>Core logic: For larger values of <code>n</code>, it iteratively calculates each term by summing the two previous terms and appends it to the list.</li> </ol>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#example-output","title":"Example Output:","text":"<p>For <code>n = 10</code>, the output will be: <pre><code>[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n</code></pre></p>"},{"location":"blog/2025/01/02/improve-your-llm-efficiency-today-------be-polite-to-your-llm/#analysis-of-generated-response_8","title":"Analysis of generated response","text":"<p>The Iterative approach is offered with no mention of other methods or efficiency. This is an example of being overly polite resulting in a more succient output without key information about alternatives.</p> <p>Connect with me on LinkedIn Follow me on Twitter</p>"},{"location":"blog/2019/05/27/an-introduction-to-convolutional-neural-networks/","title":"An introduction to Convolutional Neural Networks","text":"<p>Convolutional Neural Networks (CNNs) are specialized neural networks primarily used for image processing tasks such as classification and segmentation. They operate by applying convolutional layers that use filters, or kernels, to process input data in smaller, localized regions, effectively capturing spatial hierarchies in images. This localized approach allows CNNs to detect features like edges and textures, making them highly effective for visual data analysis.</p> <p>Beyond image processing, CNNs have been applied to various fields, including Natural Language Processing (NLP) and speech recognition. In NLP, CNNs can capture local features in text, aiding in tasks like sentiment analysis and text classification. Additionally, architectures like U-Net, which consist of mirrored CNNs forming a U-shaped structure, are utilized in applications requiring output sizes similar to inputs, such as image segmentation and enhancement.</p> <p>CNNs have also found innovative applications outside traditional domains. In genomics, for instance, they assist in detecting gene flow and inferring population size changes. In astrophysics, CNNs interpret radio telescope data to generate visual representations of celestial phenomena. Moreover, models like DeepMind's WaveNet employ CNN architectures to produce high-quality synthesized speech, demonstrating the versatility and effectiveness of CNNs across diverse scientific and technological areas.</p> <p>Read the full article here:</p> <p>An introduction to Convolutional Neural Networks</p>"},{"location":"blog/2019/03/14/u-net-deep-learning-colourisation-of-greyscale-images/","title":"U-Net deep learning colourisation of greyscale images","text":"<p>\"U-Net deep learning colourisation of greyscale images\" explores the application of deep learning techniques to transform grayscale images into colorized versions. Utilizing a U-Net architecture with a ResNet-34 encoder pretrained on ImageNet, the model employs a feature loss function based on VGG-16 activations, pixel loss, and gram matrix loss to achieve high-quality colorization. The Div2k dataset serves as the training and validation source, with data augmentation techniques such as random cropping, horizontal flipping, lighting adjustments, and perspective warping enhancing the model's robustness.</p> <p>The article demonstrates how time, resources and money can be saved with fine tuning existing models.</p> <p>The training process involves converting original RGB images to grayscale for input, with the original RGB serving as the target output. By leveraging a pretrained encoder, the model benefits from existing feature detection capabilities, significantly accelerating the training process. The feature loss function, incorporating VGG-16 activations, enables the model to capture intricate details and textures, resulting in more realistic colorization. This approach contrasts with Generative Adversarial Networks (GANs), offering faster convergence to optimal results.</p> <p>The article showcases several examples demonstrating the model's effectiveness in colorizing grayscale images, highlighting its potential applications in image restoration and enhancement. The combination of U-Net architecture, ResNet-34 encoder, and a sophisticated loss function contributes to the model's ability to produce visually compelling colorized images. The author also references related works, such as Jason Antic's DeOldify model, indicating a broader interest and ongoing development in the field of image colorization using deep learning techniques.</p> <p>Read the full article here:</p> <p>U-Net deep learning colourisation of greyscale images</p>"},{"location":"blog/2023/11/26/save-time-resources-and-money-with-latent-diffusion-based-image-generation/","title":"Save time, resources and money with Latent Diffusion based image generation.","text":"<p>This article shows a novel approach to training a generative model for image generation at reduced training times using latents and using a pre-trained ImageNet latent classifier as a component of the loss function.</p> <p>The image generation model was trained from an initialised (not pre-trained) state remarkably was less than 10 hours on a single desktop consumer NVIDIA card.</p> <p>The article delves into a novel approach to training generative models for image generation with reduced training times by leveraging latent representations and perceptual latent loss. It highlights the use of a pre-trained ImageNet latent classifier within the loss function to train a diffusion model. This method encodes over 14 million ImageNet images into latent representations using a Variational Autoencoder (VAE), dramatically reducing computational overhead. The latent classification model is trained to refine these representations, and the activations from this model are incorporated into a U-Net diffusion model to iteratively generate high-quality images from noise. The article demonstrates the efficiency and effectiveness of this technique using examples like celebrity and bedroom images, with significant improvements over traditional methods.</p> <p>The concept of perceptual latent loss is emphasized as a key innovation. Unlike conventional loss functions like MSE or MAE, this method incorporates activations from a pre-trained classifier into the loss function to achieve higher-quality outputs. Denoising Diffusion Implicit Models (DDIM) are employed for faster iterative refinement compared to the traditional Denoising Diffusion Probabilistic Models (DDPM). By progressively denoising latent representations, the model navigates from random noise back to the manifold of plausible images. This approach requires fewer steps, making it computationally efficient. The integration of DDIM and perceptual latent loss enhances the generative model's ability to produce visually coherent and detailed images.</p> <p>Lastly, the article explores the broader implications of this technique, particularly in how latent representations improve memory and computation efficiency while maintaining image quality. The methodology\u2019s success is showcased through comparisons between models trained with and without perceptual latent loss, with the former producing significantly better results. By introducing latent activations into the perceptual loss function and leveraging U-Net architectures, the approach bridges the gap between computational efficiency and high-quality image generation. This work sets a foundation for further innovations in generative AI, particularly in optimizing the training processes and enhancing output quality with limited computational resources.</p> <p>Read the full article here:</p> <p>Latent Diffusion and Perceptual Latent Loss</p>"},{"location":"blog/2020/02/21/insights-on-loss-function-engineering/","title":"Insights on loss function engineering.","text":"<p>In the realm of deep learning for image enhancement, the design of loss functions is pivotal in guiding models toward generating high-quality outputs. Traditional metrics like Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) have been widely used to measure the difference between predicted and target images. However, these pixel-based losses often lead to overly smoothed results that lack perceptual fidelity, as they tend to average out fine details, resulting in blurred images.</p> <p>To address these limitations, incorporating perceptual loss functions that leverage high-level feature representations from pretrained convolutional neural networks (CNNs) has proven effective. By comparing features extracted from intermediate layers of a CNN, models can better capture complex textures and structures, leading to more realistic and visually appealing enhancements. Additionally, adversarial losses, as employed in Generative Adversarial Networks (GANs), encourage the generation of outputs indistinguishable from real images by introducing a discriminator network that critiques the authenticity of the generated images.</p> <p>Furthermore, task-specific loss functions have been developed to cater to particular image enhancement applications. For instance, in super-resolution tasks, losses that emphasize edge preservation help maintain sharpness, while in colorization, incorporating semantic understanding ensures accurate color assignments. The engineering of these specialized loss functions, combined with advancements in network architectures, continues to drive progress in producing high-fidelity image enhancements across various domains.</p> <p>Read the full article here:</p> <p>Deep learning image enhancement insights on loss function engineering</p>"},{"location":"blog/2019/03/14/feature-based-loss-functions/","title":"Feature based loss functions","text":"<p>In this article, I explores advanced loss functions for training Convolutional Neural Networks (CNNs), particularly U-Net architectures, to enhance image generation tasks. Drawing inspiration from the Fastai deep learning course and the paper \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution,\" the discussion centers on integrating feature activation losses and style losses into the training process. These techniques aim to improve the quality of generated images by focusing on perceptual features rather than solely relying on pixel-wise errors.</p> <p>The VGG-16 model, a CNN architecture pretrained on ImageNet, plays a pivotal role in this approach. Instead of utilizing its final classification layers, the intermediate activations within the VGG-16 backbone are employed to compute feature losses. By comparing these activations between the target (ground truth) image and the generated image using mean squared error or L1 error, the model evaluates how well the generated features align with the target features. This method enables the training process to capture intricate details, leading to higher fidelity in the generated outputs.</p> <p>Additionally, the article delves into the application of Gram matrix style loss. A Gram matrix captures the style information of an image by analyzing the correlations between different feature maps. By computing the Gram matrices for both the target and generated images, the model can assess and minimize the differences in style, ensuring that the generated image not only replicates the content but also the stylistic nuances of the target. Combining feature activation losses with style losses provides a comprehensive framework for training CNNs to produce images that are both accurate in content and rich in style, enhancing the overall performance of image generation models.</p> <p>Read the full article here:</p> <p>Loss functions based on feature activation and style loss.</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/","title":"ModernBERT: Why You Should Pay Attention to the Next Generation of Encoder Models","text":"<p>Yesterday's release of ModernBERT marks a significant milestone in the evolution of encoder models, bringing much-needed modernization to one of AI's most widely deployed architectures. While large language models like GPT and Claude capture headlines, BERT-style encoders remain the backbone of countless production systems. This development is particularly relevant for organizations heavily invested in recommendation systems, search functionality, and content classification \u2013 areas where encoder models continue to be the workhorses of production systems.</p> <p>This analysis explores why ModernBERT matters and how it could change the future of production AI systems.</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#the-continuing-relevance-of-encoder-models","title":"The Continuing Relevance of Encoder Models","text":"<p>While generative AI continues to capture public imagination, encoder models remain the backbone of countless production systems. To understand ModernBERT's significance, we must first appreciate why encoder models continue to be crucial in the age of large language models (LLMs).</p> <p>Encoder models offer several compelling advantages that make them irreplaceable for many production applications:</p> <ul> <li>Significantly lower inference costs and resource requirements</li> <li>Superior processing speed for large document collections</li> <li>More predictable and controllable outputs</li> <li>Better suited for highly specific domain tasks</li> <li>Optimal efficiency-to-performance ratio for many practical applications</li> </ul> <p>These benefits explain why major technology companies like Netflix, Meta, and LinkedIn continue to rely on encoder models to power their critical infrastructure, including recommendation systems, content moderation, and semantic search capabilities.</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#modernberts-architecture","title":"ModernBERT's Architecture","text":""},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#intelligent-attention-management","title":"Intelligent Attention Management","text":"<p>The most innovative aspect of ModernBERT is its sophisticated approach to attention mechanisms. The model introduces an alternating pattern of global and local attention - much like a reader processing a novel. Every third layer employs full contextual awareness, while intermediate layers use a sliding window approach. This clever design significantly improves processing speed without compromising performance.</p> <p>Think of it this way: when reading a book, you don't need to constantly keep the entire narrative in mind to understand each sentence, but you periodically need to reflect on how the current chapter connects to the broader story. ModernBERT mirrors this natural processing pattern, leading to both improved efficiency and better understanding of context.</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#hardware-aware-design-and-efficiency-at-scale","title":"Hardware-Aware Design and Efficiency at Scale","text":"<p>ModernBERT stands out for its practical approach to deployment, with architecture optimized for common GPU configurations such as RTX 3090/4090, A10, T4, and L4. This hardware-aware design delivers several key optimizations:</p> <ul> <li>Unpadding and sequence packing eliminate wasted computation</li> <li>Integration with Flash Attention 2 and 3 maximizes memory efficiency</li> <li>Native support for 8,192 token sequences, far exceeding BERT's traditional 512 token limit</li> <li>Modified BPE tokenizer with improved efficiency for code-related tasks</li> </ul>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#modern-training-approach","title":"Modern Training Approach","text":"<p>The model's training process represents a significant leap forward, incorporating:</p> <ul> <li>Training on 2 trillion tokens, including code and scientific literature  </li> <li>An improved tokenizer based on the OLMo architecture  </li> <li>Careful preservation of special tokens for backward compatibility  </li> <li>Extensive exposure to technical documentation and code  </li> </ul>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#performance-and-business-impact","title":"Performance and Business Impact","text":""},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#benchmark-breaking-results","title":"Benchmark-Breaking Results","text":"<p>ModernBERT's performance improvements are substantial and measurable:</p> <ul> <li>First base-size model to outperform DeBERTaV3 on the GLUE benchmark</li> <li>State-of-the-art performance on code retrieval tasks</li> <li>Superior results in both single and multi-vector retrieval</li> <li>2-3x faster processing of long documents compared to existing models</li> <li>Best-in-class memory efficiency</li> <li>Exceptional code understanding capabilities, with scores exceeding 80 on the StackOverflow-QA task</li> </ul>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#practical-implementation-considerations","title":"Practical Implementation Considerations","text":"<p>For organizations considering ModernBERT adoption, several key factors deserve attention:</p> <p>Deployment Flexibility: The model is available through the Hugging Face Hub and integrates seamlessly with standard transformer pipelines. It supports fine-tuning through common frameworks like Hugging Face Transformers and Sentence-Transformers, making integration into existing systems straightforward.</p> <p>Resource Requirements: While ModernBERT offers improved efficiency over previous encoders, proper capacity planning remains important. Organizations can choose between:</p> <ul> <li>Base model (149M parameters)</li> <li>Large model (395M parameters)</li> </ul> <p>These options offer different performance-resource tradeoffs that should be evaluated based on specific use cases.</p> <p>Migration Strategy: Thanks to careful design choices in maintaining backward compatibility, organizations can implement a gradual migration strategy rather than requiring complete system rewrites.</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#strategic-considerations","title":"Strategic Considerations","text":""},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#optimal-use-cases","title":"Optimal Use Cases","text":"<p>ModernBERT might be particularly valuable for organizations that:</p> <ul> <li>Rely heavily on content classification or recommendation systems  </li> <li>Process large volumes of technical documentation or code  </li> <li>Need to optimize infrastructure costs for AI deployments  </li> <li>Require efficient processing of long documents  </li> </ul>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#current-limitations","title":"Current Limitations","text":"<p>There are these important potential limitations to be aware of:</p> <ul> <li>Currently limited to English language processing  </li> <li>May require fine-tuning for specific applications  </li> <li>May need infrastructure adjustments to fully take advantage of the efficiency gains  </li> </ul>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#looking-ahead-the-future-of-encoder-models","title":"Looking Ahead: The Future of Encoder Models","text":"<p>ModernBERT represents more than just incremental improvement - it's a carefully engineered and highly effective evolution of encoder architecture for modern computing environments. It offers an opportunity to significantly improve system performance while potentially reducing infrastructure costs.</p> <p>The model's strong performance on code-related tasks opens new possibilities for developer tooling and code search applications, making it particularly relevant for organizations building internal developer platforms or code assistance tools.</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#conclusion","title":"Conclusion","text":"<p>ModernBERT emerges as a compelling major evolution in encoder technology, bringing practical innovations that matter for production systems. Its careful balance of performance, efficiency, and compatibility makes it a serious contender for organizations looking to modernize their AI infrastructure in 2024 and beyond.</p> <p>As AI infrastructure costs continue to be a concern for many organizations, ModernBERT's efficiency-focused design provides a template for how future AI models might balance capability with practicality. For those navigating the rapidly evolving AI landscape, such practical innovations might prove more valuable than more dramatic but less immediately applicable breakthroughs.</p> <p>The combination of architectural innovations and practical optimizations positions ModernBERT as a potential new standard for production encoder models. Potential adopters would do well to evaluate it against their specific use cases, as it may provide the performance boost they seek without requiring a complete infrastructure overhaul.</p> <ul> <li>Finally, a Replacement for BERT  : The announcement of ModernBERT on Hugging Face.  </li> <li>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference  : The paper detailing ModernBERT.  </li> </ul>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#stay-connected-and-updated","title":"Stay connected and updated","text":"<p>I continuously stay informed about the latest developments and trends in AI. If you're interested in this, please connect with me or follow me.</p> <p>Connect with me on LinkedIn </p> <p>Follow me on X (Twitter)</p>"},{"location":"blog/2024/12/20/modernbert-why-you-should-pay-attention-to-the-next-generation-of-encoder-models/#resources-and-references","title":"Resources and references","text":"<ul> <li>Finally, a Replacement for BERT  : The announcement of ModernBERT on Hugging Face.  </li> <li>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference  : The paper detrailing ModernBERT.  </li> <li>ModernBERT Github repository  : The Github repository for ModernBERT.  </li> </ul>"},{"location":"blog/2019/05/12/how-do-deep-neural-networks-work/","title":"How do Deep Neural Networks work?","text":"<p>Deep neural networks (DNNs) are computational models that mimic the human brain's interconnected neuron structure to process complex data patterns. They consist of multiple layers of artificial neurons, each receiving inputs, applying weights, summing the results, and passing them through an activation function to produce an output. This layered architecture enables DNNs to model intricate relationships within data, making them effective for tasks such as image and speech recognition.</p> <p>The resurgence of interest in DNNs is largely due to advancements in computational power, particularly through the use of Graphics Processing Units (GPUs). Originally designed for rendering graphics, GPUs excel at performing rapid matrix multiplications, a fundamental operation in training neural networks. Frameworks like CUDA and cuDNN have further facilitated the deployment of neural network computations on GPUs, significantly reducing training times and enhancing performance.</p> <p>A critical component of DNNs is the activation function, which introduces non-linearity into the model, enabling it to capture complex patterns beyond linear relationships. While early neural networks often utilized sigmoid functions, modern architectures typically employ Rectified Linear Units (ReLUs) or their variants. ReLUs output zero for negative inputs and pass positive inputs unchanged, allowing models to learn complex data representations more effectively.</p> <p>Read the full article here:</p> <p>The basics of Deep Neural Networks</p>"},{"location":"blog/2019/02/11/random-forests---a-free-lunch-thats-not-cursed/","title":"Random forests - a free lunch that\u2019s not cursed.","text":"<p>Random forests are a powerful machine learning technique that combines multiple decision trees to enhance predictive accuracy and control overfitting. By aggregating the results of various trees, random forests mitigate the risk of individual trees capturing noise from the training data, leading to more robust and reliable models.</p> <p>One of the key advantages of random forests is their ability to handle high-dimensional data effectively. They can manage a large number of input variables without the need for variable deletion, making them suitable for complex datasets. Additionally, random forests provide estimates of feature importance, aiding in the identification of the most influential variables in a dataset.</p> <p>Despite their strengths, it's essential to be mindful of certain limitations when using random forests. They can be computationally intensive, especially with large datasets, and may require careful tuning of parameters to achieve optimal performance. Nonetheless, with appropriate implementation, random forests serve as a versatile and effective tool for both classification and regression tasks in machine learning.</p> <p>Read the full article here:</p> <p>Random Forests - a free lunch that's not cursed</p>"},{"location":"blog/2021/01/31/rapid-prototyping-of-network-architectures-using-super-convergence-using-cyclical-learning-rate-schedules/","title":"Rapid prototyping of network architectures using Super-Convergence using Cyclical Learning Rate schedules.","text":"<p>Super-convergence, achieved through cyclical learning rates, is a powerful yet underutilized technique in deep learning that significantly accelerates model training. By varying the learning rate between high and low boundaries, models can converge in a fraction of the time typically required. This method facilitates rapid prototyping of network architectures, optimization of loss functions, and experimentation with data augmentation, all while reducing training time by orders of magnitude.</p> <p>Implementing cyclical learning rates enables training complex models, such as those for super-resolution tasks, from scratch in mere minutes without relying on pre-trained weights. For instance, a state-of-the-art super-resolution model was trained in just 16 epochs\u2014approximately four minutes\u2014using a learning rate cycling between 0.007 and 0.0007, achieving impressive results on the DIV2K dataset. This approach challenges the conventional practice of using moderate, fixed learning rates over thousands of epochs, demonstrating that higher learning rates, when applied cyclically, can lead to faster convergence without causing instability.</p> <p>Adopting cyclical learning rates not only enhances training efficiency but also offers practical benefits, such as reduced computational costs and energy consumption\u2014particularly advantageous when utilizing cloud infrastructure. Moreover, this technique allows researchers to conduct a greater number of experiments in less time, thereby accelerating the development and refinement of deep learning models. Despite its advantages, cyclical learning rates remain underexploited in the deep learning community, presenting an opportunity for practitioners to improve model performance and training speed by integrating this strategy into their workflows.</p> <p>Read the full article here:</p> <p>Super Convergence with Cyclical Learning Rates in TensorFlow</p>"},{"location":"blog/2021/03/24/super-resolution-adobe-photoshop-versus-leading-deep-neural-networks/","title":"Super Resolution: Adobe Photoshop versus Leading Deep Neural Networks.","text":"<p>Super Resolution is a technique that enhances the quality of an image by increasing its apparent resolution, effectively imagining the detail present in a higher-resolution version. Traditional methods like bicubic interpolation often result in blurred images when upscaling. Recent advancements have introduced more sophisticated approaches, including Adobe Camera Raw's Super Resolution and deep learning models such as the Information Distillation Network (IDN).</p> <p>Adobe's Super Resolution, integrated into Adobe Camera Raw and Photoshop, utilizes an advanced machine learning model trained on millions of photos. This feature allows users to enhance image resolution with a single click, delivering impressive results in both performance and speed. While Adobe has not disclosed detailed technical specifics, the algorithm's effectiveness suggests the use of deep neural network techniques.</p> <p>The Information Distillation Network (IDN) represents a deep convolutional neural network architecture developed by researchers Zheng Hui, Xiumei Wang, and Xinbo Gao. IDN is recognized for its fast and accurate single-image super-resolution capabilities, generalizing well across various images. Unlike earlier models that focused on small 'postage stamp' images, IDN has been evaluated on relatively high-resolution images, demonstrating its applicability to more substantial image enhancement tasks.</p> <p>Read the full article here:</p> <p>Super Resolution: Adobe Photoshop versus Leading Deep Neural Networks</p>"},{"location":"blog/2019/02/24/deep-learning-based-super-resolution-without-using-a-gan/","title":"Deep learning based super resolution, without using a GAN.","text":"<p>Super-resolution is a technique that enhances the quality and detail of low-resolution images, effectively transforming them into higher-resolution versions. Traditional upscaling methods often result in images lacking fine details and may introduce defects or compression artifacts. Deep learning approaches, particularly those utilizing Generative Adversarial Networks (GANs), have shown significant improvements in this area. However, training GANs can be complex and resource-intensive.</p> <p>In this article, we explore an alternative deep learning method for super-resolution that does not rely on GANs. By leveraging techniques from the Fastai library and course, we train a model capable of enhancing image resolution effectively. This approach simplifies the training process while still delivering impressive results in image restoration and inpainting. The methodology combines various advanced techniques, some of which are relatively unique in their application as of early 2019.</p> <p>The practical benefits of this GAN-free super-resolution method are substantial. It enables the recovery of high-quality images from low-resolution inputs, which is invaluable in fields such as medical imaging, where enhanced image clarity can be life-saving. Additionally, it offers potential for efficient data transmission by allowing the transfer of lower-resolution images that can be upscaled upon receipt, optimizing bandwidth usage. This method provides a more accessible and less resource-demanding solution for image enhancement tasks.</p> <p>Read the full article here:</p> <p>Deep learning based super resolution, without using a GAN</p>"},{"location":"blog/2019/09/09/tabular-data-analysis-with-deep-neural-nets/","title":"Tabular data analysis with deep neural nets.","text":"<p>Deep neural networks (DNNs) have emerged as a powerful tool for analyzing tabular data, offering advantages over traditional methods like Random Forests and Gradient Boosting Machines. Unlike these conventional techniques, DNNs require minimal feature engineering and maintenance, making them suitable for various applications, including fraud detection, sales forecasting, and credit risk assessment. Notably, companies like Pinterest have transitioned to neural networks from gradient boosting machines, citing improved accuracy and reduced need for feature engineering.</p> <p>In tabular data analysis, datasets typically comprise continuous variables (e.g., age, weight) and categorical variables (e.g., marital status, dog breed). While DNNs can process continuous data directly, preprocessing steps are essential to handle missing values and normalize data. For instance, missing continuous values can be replaced with the median, and an additional feature can indicate the absence of data, ensuring the model accounts for missing information without biasing predictions.</p> <p>Despite the reduced need for extensive feature engineering, careful preprocessing remains crucial when employing DNNs for tabular data. This includes normalizing continuous variables and appropriately encoding categorical variables to ensure the model effectively captures underlying patterns. Additionally, ethical considerations should be addressed, particularly regarding features that may introduce bias or discrimination into the model. By adhering to these practices, DNNs can serve as reliable and efficient tools for a wide range of tabular data analysis tasks.</p> <p>Read the full article here:</p> <p>Tabular data analysis with deep neural nets</p>"},{"location":"blog/2019/03/14/u-nets-with-resnet-encoders-and-cross-connections/","title":"U-Nets with ResNet Encoders and cross connections","text":"<p>In this article, the author explores an advanced U-Net architecture that integrates ResNet encoders and cross connections, enhancing the model's performance in image processing tasks. This design incorporates elements from DenseNets, utilizing cross connections to facilitate efficient information flow between layers. The architecture employs a ResNet-based encoder and decoder, complemented by pixel shuffle upscaling with ICNR initialization, aiming to improve prediction accuracy and training efficiency.</p> <p>The article delves into the functionalities of Residual Networks (ResNets) and their constituent residual blocks (ResBlocks). ResNets address the vanishing gradient problem in deep neural networks through skip connections, enabling the construction of deeper, more accurate models. The author explains how these skip connections create a more navigable loss surface, facilitating effective training of the network.</p> <p>Additionally, the author discusses Densely Connected Convolutional Networks (DenseNets) and their dense blocks, which utilize tensor concatenation to allow computation to bypass larger sections of the architecture. While DenseBlocks can be memory-intensive, they are particularly effective for smaller datasets. The article also covers the U-Net architecture, originally developed for biomedical image segmentation, highlighting its effectiveness in tasks requiring outputs of similar size to the inputs.</p> <p>Read the full article here:</p> <p>U-Nets with ResNet Encoders and cross connections</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/page/2/","title":"Blog","text":""}]}