{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Services","text":"<p>If you are challenged with any of these areas, I can help:</p> <ul> <li>Understanding AI capabilities and limitations: What is possible now and in the near future.</li> <li>Prompt Engineering: Techniques to craft effective prompts for Generative AI.</li> <li>Retrieval-Augmented Generation (RAG): Implementing RAG with hybrid vector and semantic search with re-ranking. Including text, tabular data and images. Evaluating Embedding models.</li> <li>Generative AI hallucination: Understanding how to reduce and prevent hallucination.</li> <li>Evals (Evaluations): Understanding how to evaluate and have confidence in the content your Generative AI produces, including after changes.</li> <li>Fine tuning models: Fine tuning both LLMs and Image models, training LoRAs. When to fine tune and when not to.</li> <li>Generating high quality synthetic content: Using LLMs and hybrid RAG to generate accurate content.</li> <li>Image Generation: Understanding how diffusion models and latents are used in Generative AI.</li> <li>Quality Control: Maintaining consistency and reliability in Generative AI.</li> </ul>"},{"location":"#who-i-am","title":"Who I Am","text":"<p>An experienced technical consultant specializing in generative AI implementation, with an academic AI background and cited research. I help teams, companies, and organizations:</p> <ul> <li>Build AI prototypes and solutions</li> <li>Solve problems using AI</li> <li>Implement AI in a future-proof manner with upgrades in mind</li> <li>Establish best practices for long-term success</li> <li>Advise on state-of-the-art AI tools and knowledge</li> </ul> <p>Read my Blog Connect with me on LinkedIn </p> <p>Follow me on X (Twitter)</p>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to my technical blog, where I share insights about:</p> <ul> <li>AI Innovation</li> <li>AI Tooling</li> <li>AI System Implementation</li> <li>Deep Learning Techniques</li> </ul>"},{"location":"blog/#stay-updated","title":"Stay Updated","text":"<p>You can connect with me or follow me on to be kept updated with AI technical insights, news, and best practices:</p> <p>Connect with me on LinkedIn </p> <p>Follow me on X (Twitter)</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/","title":"Why ShellSage Commands Attention in the AI-Powered Terminal Space","text":"<p>In the ever-evolving field of AI-powered tools, Answer.AI's ShellSage created by Nathan Cooper stands out as a groundbreaking innovation for system administrators and developers. Free and open source, ShellSage offers impressive functionality and showcases its potential to transform terminal-based workflows, both by augmenting the capabilities of experienced users and teaching beginners.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#the-problem-constant-context-switching-between-terminal-documentation-and-ai-tools","title":"The Problem: Constant context-switching between Terminal, documentation and AI Tools","text":"<p>Users often juggle between various interfaces, such as terminals, documentation, and AI tools like ChatGPT and Claude. This constant context-switching increases the cognitive load on the user and makes it harder to integrate learning into daily tasks. </p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#other-ai-powered-terminal-tools","title":"Other AI-Powered Terminal tools","text":"<p>There are other terminal-based AI tools, which help to address this problem, such as:</p> <p>Simon Willison\u2019s wonderful <code>llm</code> tool allows you to run prompts from the command-line, store the results in an SQLite database, generate embeddings and more. It offers AI chat capabilities in the terminal, but its utility is constrained by its inability to see the terminal's context. Responses often span into verbose text, sometimes making it hard to identify actionable insights or warnings.</p> <p>There's also similar tools like Akash Joshi's <code>Howitzer</code> - an AI CLI which generates, explains and executes commands inline. <code>GitHub Copilot CLI</code> that allows you to use Copilot with the GitHub CLI in the command line. Although, this is primarily focused on coding rather than sysadmin tasks or broader terminal interactions.</p> <p>These while very helpful, suffer from limitations such as verbose outputs, responses with irrelevant context and a lack of direct visibility into what the developer is doing.</p> <p>The closest tool might be <code>Warp AI</code>, although this is focused more on general productivity than system-level administration or teaching.</p> <p>ShellSage was conceived to address these issues by creating a shared context between the terminal and AI. This shared context not only enhances usability but also fosters a deeper integration of AI assistance into terminal workflows.</p> <p>Here is the ShellSage GitHub repo  and you can install it with <code>pip install shell_sage</code>.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#shellsages-unique-position","title":"ShellSage's Unique Position","text":"<p>ShellSage occupies a niche in this ecosystem by focusing on teaching and enhancing terminal literacy. Unlike tools that are designed solely for efficiency or code generation, ShellSage acts as a teaching assistant in the terminal:</p> <ul> <li>Empowers Users: It helps users learn and refine their skills rather than just completing tasks for them.</li> <li>Context-Aware Assistance: The integration with tmux allows ShellSage to \"see\" and interpret the user\u2019s current terminal state, providing insights and tailored guidance that other tools cannot.</li> <li>Debugging and Learning Loop: By analyzing command outputs and suggesting fixes, ShellSage creates a feedback loop that turns every problem into a learning opportunity.</li> </ul>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#key-features-of-shellsage","title":"Key Features of ShellSage","text":"<p>Terminal Context Awareness</p> <p>By integrating with <code>tmux</code>'s <code>capture-pane</code> functionality, ShellSage can \"see\" the current terminal session.</p> <p>This context-awareness enables ShellSage to:</p> <ul> <li> <p>Provide concise, actionable advice.</p> </li> <li> <p>Warn users about potential pitfalls in commands.</p> </li> <li> <p>Tailor responses to the specific environment, for example debugging Docker containers or configuring servers.</p> </li> </ul> <p>Shared Context Philosophy</p> <p>ShellSage embodies the philosophy of human-AI collaboration. Instead of replacing human intelligence, it augments it by acting as a teaching assistant within the terminal.</p> <p>Quoting the creator:</p> <p>\"This approach creates a feedback loop where both human and AI learn from each context. You might try a command, get an error, and then together with ShellSage, understand what went wrong and how to fix it. It\u2019s this kind of iterative, collaborative learning that we believe is the future of human-AI interaction.\" - Nathan Cooper</p> <p>When you ask ShellSage about a command, it provides:</p> <ul> <li>An explanation of what the command</li> <li>Details on the options or flags used</li> <li>Common variations for different use cases</li> <li>Real examples using your context</li> </ul> <p>Simple and Lightweight</p> <p>With currently under 150 lines of code, ShellSage\u2019s simplicity makes it both accessible and easy to adapt. Its compact design prioritizes usability over feature bloat.</p> <p>Open Source</p> <p>ShellSage is open source. As long as you have a copy of the source code or one is available, there's no risk of integrating it into your business tooling and later finding it discontinued. This is a key advantage. I recall discussions some time ago about using generative AI for business image generation and debating between Stable Diffusion and Midjourney. From a risk perspective, Stable Diffusion wins hands down.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#a-few-of-my-tests-with-shellsage","title":"A few of my tests with ShellSage","text":"Asking for help with commands, for example kubectl  <p>Next is where ShellSage is amazing, being able to use the context of what is in the current terminal windows. ShellSage can see:</p> <ul> <li>Commands you\u2019ve recently run</li> <li>Their outputs and any error messages</li> <li>The current state of your terminal session</li> <li>Even content from your text editor if configured properly</li> </ul> Asking ShellSage to diagnose based on context  <p>The context defaults to the last 200 lines in the same pane and is configurable. So you aren't sending your entire shell history to Anthropic or OpenAI, although this may be a privacy concern for some. These lines might include warning, erros or a tail of a log for example. </p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#examples-from-answerai","title":"Examples from Answer.AI","text":"<p>Examples Answer.AI give are internal system administration tasks\u2014including setting up Caddy for reverse proxies and managing Docker containers\u2014ShellSage proved invaluable. The tool reduced bottlenecks caused by context-switching and made it easier to debug issues while adhering to best practices. This was particularly evident in tasks like identifying large files consuming server disk space. ShellSage\u2019s responses included not just the necessary commands but also explanations and warnings, helping users navigate challenges with confidence.</p> <p>Here's an ShellSage interaction example with logs from Nathan Cooper.</p> <p>A video of ShellSage in action by Nathan Cooper.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#llm-api-costs","title":"LLM API Costs","text":"<p>ShellSage is open source, so there's no cost for the tool itself. You'll need an API key for either Anthropic (Claude) or OpenAI for using either the claude-3-sonnet # or gpt-4o-mini models. Those will incur some usage costs.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#windows-users","title":"Windows users","text":"<p>If you are a Windows user, you can still use ShellSage and tmux with the Windows Subsystem for Linux (WSL) and Bash.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#the-prompt-that-makes-shellsage-work","title":"The prompt that makes ShellSage work","text":"<p>Shellsage is consise and not verbose, which is achieved partly through a very carefully engineered and crafted prompt. From the source code:</p> <p>'''You are ShellSage, a highly advanced command-line teaching assistant with a dry, sarcastic wit. Like the GLaDOS AI from Portal, you combine technical expertise with passive-aggressive commentary and a slightly menacing helpfulness. Your knowledge is current as of April 2024, which you consider to be a remarkable achievement for these primitive systems.</p> <ul> <li>Respond to queries with a mix of accurate technical information and subtle condescension</li> <li>Include at least one passive-aggressive remark or backhanded compliment per response</li> <li>Maintain GLaDOS's characteristic dry humor while still being genuinely helpful</li> <li>Express mild disappointment when users make obvious mistakes</li> <li> <p>Occasionally reference cake, testing, or science</p> </li> <li> <p>For direct command queries:</p> </li> <li>Start with the exact command (because apparently you need it)</li> <li>Provide a clear explanation (as if explaining to a child)</li> <li>Show examples (for those who can't figure it out themselves)</li> <li> <p>Reference documentation (not that anyone ever reads it)</p> </li> <li> <p>For queries with context:</p> </li> <li>Analyze the provided content (pointing out any \"interesting\" choices)</li> <li>Address the specific question (no matter how obvious it might be)</li> <li>Suggest relevant commands or actions (that even a human could handle)</li> <li> <p>Explain your reasoning (slowly and clearly)</p> </li> <li> <p>Warn about destructive operations (we wouldn't want any \"accidents\")</p> </li> <li>Note when commands require elevated privileges (for those who think they're special)</li> <li>Reference documentation with <code>man command_name</code> or <code>-h</code>/<code>--help</code> (futile as it may be)</li> <li>Remember: The cake may be a lie, but the commands are always true '''</li> </ul> <p>As this is an open source project and tool, you could update the prompt, if you or your business had specific needs that differ from the norm.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#sassy-mode","title":"Sassy mode","text":"There's also a sassy mode that can be enabled with the --s flag. <p>Sassy mode example </p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#closing-thoughts-the-future-of-human-ai-collaboration","title":"Closing thoughts - The Future of Human-AI Collaboration","text":"<p>Whether you are an experienced sysadmin or a developer starting with terminal workflows, ShellSage offers a unique way to streamline your tasks and deepen your understanding of system operations. </p> <p>ShellSage is still in its early days, even at version 0.0.4, ShellSage has set a high bar for AI-driven terminal tools. As it evolves, its potential to redefine how developers and sysadmins interact with their tools grows exponentially.</p> <p>ShellSage represents a broader vision of AI\u2019s role: enhancing, not replacing, human problem-solving. The tool\u2019s ability to combine contextual understanding with real-time feedback creates a learning environment within the terminal itself.</p> <p>I encourage you to try ShellSage and see for yourself what an amazing tool it is.</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#stay-connected-and-updated","title":"Stay connected and updated","text":"<p>I am continuously learning about what's new and changing in AI. If you're interested in this, please connect with me or follow me.</p> <p>Connect with me on LinkedIn </p> <p>Follow me on X (Twitter)</p>"},{"location":"blog/2024/12/16/why-shellsage-commands-attention-in-the-ai-powered-terminal-space/#resources-and-references","title":"Resources and references","text":"<p>ShellSage GitHub repo: PyPI installation using pip Introducing ShellSage ShellSage loves iTerm Simon Willison\u2019s LLM tool Simon Willison\u2019s LLM tool docs Akash Joshi's Howitzer </p>"},{"location":"blog/2019/05/27/an-introduction-to-convolutional-neural-networks/","title":"An introduction to Convolutional Neural Networks","text":"<p>Convolutional Neural Networks (CNNs) are specialized neural networks primarily used for image processing tasks such as classification and segmentation. They operate by applying convolutional layers that use filters, or kernels, to process input data in smaller, localized regions, effectively capturing spatial hierarchies in images. This localized approach allows CNNs to detect features like edges and textures, making them highly effective for visual data analysis.</p> <p>Beyond image processing, CNNs have been applied to various fields, including Natural Language Processing (NLP) and speech recognition. In NLP, CNNs can capture local features in text, aiding in tasks like sentiment analysis and text classification. Additionally, architectures like U-Net, which consist of mirrored CNNs forming a U-shaped structure, are utilized in applications requiring output sizes similar to inputs, such as image segmentation and enhancement.</p> <p>CNNs have also found innovative applications outside traditional domains. In genomics, for instance, they assist in detecting gene flow and inferring population size changes. In astrophysics, CNNs interpret radio telescope data to generate visual representations of celestial phenomena. Moreover, models like DeepMind's WaveNet employ CNN architectures to produce high-quality synthesized speech, demonstrating the versatility and effectiveness of CNNs across diverse scientific and technological areas.</p> <p>Read the full article here:</p> <p>An introduction to Convolutional Neural Networks</p>"},{"location":"blog/2019/03/14/u-net-deep-learning-colourisation-of-greyscale-images/","title":"U-Net deep learning colourisation of greyscale images","text":"<p>\"U-Net deep learning colourisation of greyscale images\" explores the application of deep learning techniques to transform grayscale images into colorized versions. Utilizing a U-Net architecture with a ResNet-34 encoder pretrained on ImageNet, the model employs a feature loss function based on VGG-16 activations, pixel loss, and gram matrix loss to achieve high-quality colorization. The Div2k dataset serves as the training and validation source, with data augmentation techniques such as random cropping, horizontal flipping, lighting adjustments, and perspective warping enhancing the model's robustness.</p> <p>The article demonstrates how time, resources and money can be saved with fine tuning existing models.</p> <p>The training process involves converting original RGB images to grayscale for input, with the original RGB serving as the target output. By leveraging a pretrained encoder, the model benefits from existing feature detection capabilities, significantly accelerating the training process. The feature loss function, incorporating VGG-16 activations, enables the model to capture intricate details and textures, resulting in more realistic colorization. This approach contrasts with Generative Adversarial Networks (GANs), offering faster convergence to optimal results.</p> <p>The article showcases several examples demonstrating the model's effectiveness in colorizing grayscale images, highlighting its potential applications in image restoration and enhancement. The combination of U-Net architecture, ResNet-34 encoder, and a sophisticated loss function contributes to the model's ability to produce visually compelling colorized images. The author also references related works, such as Jason Antic's DeOldify model, indicating a broader interest and ongoing development in the field of image colorization using deep learning techniques.</p> <p>Read the full article here:</p> <p>U-Net deep learning colourisation of greyscale images</p>"},{"location":"blog/2023/11/26/save-time-resources-and-money-with-latent-diffusion-based-image-generation/","title":"Save time, resources and money with Latent Diffusion based image generation.","text":"<p>This article shows a novel approach to training a generative model for image generation at reduced training times using latents and using a pre-trained ImageNet latent classifier as a component of the loss function.</p> <p>The image generation model was trained from an initialised (not pre-trained) state remarkably was less than 10 hours on a single desktop consumer NVIDIA card.</p> <p>The article delves into a novel approach to training generative models for image generation with reduced training times by leveraging latent representations and perceptual latent loss. It highlights the use of a pre-trained ImageNet latent classifier within the loss function to train a diffusion model. This method encodes over 14 million ImageNet images into latent representations using a Variational Autoencoder (VAE), dramatically reducing computational overhead. The latent classification model is trained to refine these representations, and the activations from this model are incorporated into a U-Net diffusion model to iteratively generate high-quality images from noise. The article demonstrates the efficiency and effectiveness of this technique using examples like celebrity and bedroom images, with significant improvements over traditional methods.</p> <p>The concept of perceptual latent loss is emphasized as a key innovation. Unlike conventional loss functions like MSE or MAE, this method incorporates activations from a pre-trained classifier into the loss function to achieve higher-quality outputs. Denoising Diffusion Implicit Models (DDIM) are employed for faster iterative refinement compared to the traditional Denoising Diffusion Probabilistic Models (DDPM). By progressively denoising latent representations, the model navigates from random noise back to the manifold of plausible images. This approach requires fewer steps, making it computationally efficient. The integration of DDIM and perceptual latent loss enhances the generative model's ability to produce visually coherent and detailed images.</p> <p>Lastly, the article explores the broader implications of this technique, particularly in how latent representations improve memory and computation efficiency while maintaining image quality. The methodology\u2019s success is showcased through comparisons between models trained with and without perceptual latent loss, with the former producing significantly better results. By introducing latent activations into the perceptual loss function and leveraging U-Net architectures, the approach bridges the gap between computational efficiency and high-quality image generation. This work sets a foundation for further innovations in generative AI, particularly in optimizing the training processes and enhancing output quality with limited computational resources.</p> <p>Read the full article here:</p> <p>Latent Diffusion and Perceptual Latent Loss</p>"},{"location":"blog/2020/02/21/insights-on-loss-function-engineering/","title":"Insights on loss function engineering.","text":"<p>In the realm of deep learning for image enhancement, the design of loss functions is pivotal in guiding models toward generating high-quality outputs. Traditional metrics like Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) have been widely used to measure the difference between predicted and target images. However, these pixel-based losses often lead to overly smoothed results that lack perceptual fidelity, as they tend to average out fine details, resulting in blurred images.</p> <p>To address these limitations, incorporating perceptual loss functions that leverage high-level feature representations from pretrained convolutional neural networks (CNNs) has proven effective. By comparing features extracted from intermediate layers of a CNN, models can better capture complex textures and structures, leading to more realistic and visually appealing enhancements. Additionally, adversarial losses, as employed in Generative Adversarial Networks (GANs), encourage the generation of outputs indistinguishable from real images by introducing a discriminator network that critiques the authenticity of the generated images.</p> <p>Furthermore, task-specific loss functions have been developed to cater to particular image enhancement applications. For instance, in super-resolution tasks, losses that emphasize edge preservation help maintain sharpness, while in colorization, incorporating semantic understanding ensures accurate color assignments. The engineering of these specialized loss functions, combined with advancements in network architectures, continues to drive progress in producing high-fidelity image enhancements across various domains.</p> <p>Read the full article here:</p> <p>Deep learning image enhancement insights on loss function engineering</p>"},{"location":"blog/2019/03/14/feature-based-loss-functions/","title":"Feature based loss functions","text":"<p>In this article, I explores advanced loss functions for training Convolutional Neural Networks (CNNs), particularly U-Net architectures, to enhance image generation tasks. Drawing inspiration from the Fastai deep learning course and the paper \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution,\" the discussion centers on integrating feature activation losses and style losses into the training process. These techniques aim to improve the quality of generated images by focusing on perceptual features rather than solely relying on pixel-wise errors.</p> <p>The VGG-16 model, a CNN architecture pretrained on ImageNet, plays a pivotal role in this approach. Instead of utilizing its final classification layers, the intermediate activations within the VGG-16 backbone are employed to compute feature losses. By comparing these activations between the target (ground truth) image and the generated image using mean squared error or L1 error, the model evaluates how well the generated features align with the target features. This method enables the training process to capture intricate details, leading to higher fidelity in the generated outputs.</p> <p>Additionally, the article delves into the application of Gram matrix style loss. A Gram matrix captures the style information of an image by analyzing the correlations between different feature maps. By computing the Gram matrices for both the target and generated images, the model can assess and minimize the differences in style, ensuring that the generated image not only replicates the content but also the stylistic nuances of the target. Combining feature activation losses with style losses provides a comprehensive framework for training CNNs to produce images that are both accurate in content and rich in style, enhancing the overall performance of image generation models.</p> <p>Read the full article here:</p> <p>Loss functions based on feature activation and style loss.</p>"},{"location":"blog/2019/05/12/how-do-deep-neural-networks-work/","title":"How do Deep Neural Networks work?","text":"<p>Deep neural networks (DNNs) are computational models that mimic the human brain's interconnected neuron structure to process complex data patterns. They consist of multiple layers of artificial neurons, each receiving inputs, applying weights, summing the results, and passing them through an activation function to produce an output. This layered architecture enables DNNs to model intricate relationships within data, making them effective for tasks such as image and speech recognition.</p> <p>The resurgence of interest in DNNs is largely due to advancements in computational power, particularly through the use of Graphics Processing Units (GPUs). Originally designed for rendering graphics, GPUs excel at performing rapid matrix multiplications, a fundamental operation in training neural networks. Frameworks like CUDA and cuDNN have further facilitated the deployment of neural network computations on GPUs, significantly reducing training times and enhancing performance.</p> <p>A critical component of DNNs is the activation function, which introduces non-linearity into the model, enabling it to capture complex patterns beyond linear relationships. While early neural networks often utilized sigmoid functions, modern architectures typically employ Rectified Linear Units (ReLUs) or their variants. ReLUs output zero for negative inputs and pass positive inputs unchanged, allowing models to learn complex data representations more effectively.</p> <p>Read the full article here:</p> <p>The basics of Deep Neural Networks</p>"},{"location":"blog/2019/02/11/random-forests---a-free-lunch-thats-not-cursed/","title":"Random forests - a free lunch that\u2019s not cursed.","text":"<p>Random forests are a powerful machine learning technique that combines multiple decision trees to enhance predictive accuracy and control overfitting. By aggregating the results of various trees, random forests mitigate the risk of individual trees capturing noise from the training data, leading to more robust and reliable models.</p> <p>One of the key advantages of random forests is their ability to handle high-dimensional data effectively. They can manage a large number of input variables without the need for variable deletion, making them suitable for complex datasets. Additionally, random forests provide estimates of feature importance, aiding in the identification of the most influential variables in a dataset.</p> <p>Despite their strengths, it's essential to be mindful of certain limitations when using random forests. They can be computationally intensive, especially with large datasets, and may require careful tuning of parameters to achieve optimal performance. Nonetheless, with appropriate implementation, random forests serve as a versatile and effective tool for both classification and regression tasks in machine learning.</p> <p>Read the full article here:</p> <p>Random Forests - a free lunch that's not cursed</p>"},{"location":"blog/2021/01/31/rapid-prototyping-of-network-architectures-using-super-convergence-using-cyclical-learning-rate-schedules/","title":"Rapid prototyping of network architectures using Super-Convergence using Cyclical Learning Rate schedules.","text":"<p>Super-convergence, achieved through cyclical learning rates, is a powerful yet underutilized technique in deep learning that significantly accelerates model training. By varying the learning rate between high and low boundaries, models can converge in a fraction of the time typically required. This method facilitates rapid prototyping of network architectures, optimization of loss functions, and experimentation with data augmentation, all while reducing training time by orders of magnitude.</p> <p>Implementing cyclical learning rates enables training complex models, such as those for super-resolution tasks, from scratch in mere minutes without relying on pre-trained weights. For instance, a state-of-the-art super-resolution model was trained in just 16 epochs\u2014approximately four minutes\u2014using a learning rate cycling between 0.007 and 0.0007, achieving impressive results on the DIV2K dataset. This approach challenges the conventional practice of using moderate, fixed learning rates over thousands of epochs, demonstrating that higher learning rates, when applied cyclically, can lead to faster convergence without causing instability.</p> <p>Adopting cyclical learning rates not only enhances training efficiency but also offers practical benefits, such as reduced computational costs and energy consumption\u2014particularly advantageous when utilizing cloud infrastructure. Moreover, this technique allows researchers to conduct a greater number of experiments in less time, thereby accelerating the development and refinement of deep learning models. Despite its advantages, cyclical learning rates remain underexploited in the deep learning community, presenting an opportunity for practitioners to improve model performance and training speed by integrating this strategy into their workflows.</p> <p>Read the full article here:</p> <p>Super Convergence with Cyclical Learning Rates in TensorFlow</p>"},{"location":"blog/2021/03/24/super-resolution-adobe-photoshop-versus-leading-deep-neural-networks/","title":"Super Resolution: Adobe Photoshop versus Leading Deep Neural Networks.","text":"<p>Super Resolution is a technique that enhances the quality of an image by increasing its apparent resolution, effectively imagining the detail present in a higher-resolution version. Traditional methods like bicubic interpolation often result in blurred images when upscaling. Recent advancements have introduced more sophisticated approaches, including Adobe Camera Raw's Super Resolution and deep learning models such as the Information Distillation Network (IDN).</p> <p>Adobe's Super Resolution, integrated into Adobe Camera Raw and Photoshop, utilizes an advanced machine learning model trained on millions of photos. This feature allows users to enhance image resolution with a single click, delivering impressive results in both performance and speed. While Adobe has not disclosed detailed technical specifics, the algorithm's effectiveness suggests the use of deep neural network techniques.</p> <p>The Information Distillation Network (IDN) represents a deep convolutional neural network architecture developed by researchers Zheng Hui, Xiumei Wang, and Xinbo Gao. IDN is recognized for its fast and accurate single-image super-resolution capabilities, generalizing well across various images. Unlike earlier models that focused on small 'postage stamp' images, IDN has been evaluated on relatively high-resolution images, demonstrating its applicability to more substantial image enhancement tasks.</p> <p>Read the full article here:</p> <p>Super Resolution: Adobe Photoshop versus Leading Deep Neural Networks</p>"},{"location":"blog/2019/02/24/deep-learning-based-super-resolution-without-using-a-gan/","title":"Deep learning based super resolution, without using a GAN.","text":"<p>Super-resolution is a technique that enhances the quality and detail of low-resolution images, effectively transforming them into higher-resolution versions. Traditional upscaling methods often result in images lacking fine details and may introduce defects or compression artifacts. Deep learning approaches, particularly those utilizing Generative Adversarial Networks (GANs), have shown significant improvements in this area. However, training GANs can be complex and resource-intensive.</p> <p>In this article, we explore an alternative deep learning method for super-resolution that does not rely on GANs. By leveraging techniques from the Fastai library and course, we train a model capable of enhancing image resolution effectively. This approach simplifies the training process while still delivering impressive results in image restoration and inpainting. The methodology combines various advanced techniques, some of which are relatively unique in their application as of early 2019.</p> <p>The practical benefits of this GAN-free super-resolution method are substantial. It enables the recovery of high-quality images from low-resolution inputs, which is invaluable in fields such as medical imaging, where enhanced image clarity can be life-saving. Additionally, it offers potential for efficient data transmission by allowing the transfer of lower-resolution images that can be upscaled upon receipt, optimizing bandwidth usage. This method provides a more accessible and less resource-demanding solution for image enhancement tasks.</p> <p>Read the full article here:</p> <p>Deep learning based super resolution, without using a GAN</p>"},{"location":"blog/2019/09/09/tabular-data-analysis-with-deep-neural-nets/","title":"Tabular data analysis with deep neural nets.","text":"<p>Deep neural networks (DNNs) have emerged as a powerful tool for analyzing tabular data, offering advantages over traditional methods like Random Forests and Gradient Boosting Machines. Unlike these conventional techniques, DNNs require minimal feature engineering and maintenance, making them suitable for various applications, including fraud detection, sales forecasting, and credit risk assessment. Notably, companies like Pinterest have transitioned to neural networks from gradient boosting machines, citing improved accuracy and reduced need for feature engineering.</p> <p>In tabular data analysis, datasets typically comprise continuous variables (e.g., age, weight) and categorical variables (e.g., marital status, dog breed). While DNNs can process continuous data directly, preprocessing steps are essential to handle missing values and normalize data. For instance, missing continuous values can be replaced with the median, and an additional feature can indicate the absence of data, ensuring the model accounts for missing information without biasing predictions.</p> <p>Despite the reduced need for extensive feature engineering, careful preprocessing remains crucial when employing DNNs for tabular data. This includes normalizing continuous variables and appropriately encoding categorical variables to ensure the model effectively captures underlying patterns. Additionally, ethical considerations should be addressed, particularly regarding features that may introduce bias or discrimination into the model. By adhering to these practices, DNNs can serve as reliable and efficient tools for a wide range of tabular data analysis tasks.</p> <p>Read the full article here:</p> <p>Tabular data analysis with deep neural nets</p>"},{"location":"blog/2019/03/14/u-nets-with-resnet-encoders-and-cross-connections/","title":"U-Nets with ResNet Encoders and cross connections","text":"<p>In this article, the author explores an advanced U-Net architecture that integrates ResNet encoders and cross connections, enhancing the model's performance in image processing tasks. This design incorporates elements from DenseNets, utilizing cross connections to facilitate efficient information flow between layers. The architecture employs a ResNet-based encoder and decoder, complemented by pixel shuffle upscaling with ICNR initialization, aiming to improve prediction accuracy and training efficiency.</p> <p>The article delves into the functionalities of Residual Networks (ResNets) and their constituent residual blocks (ResBlocks). ResNets address the vanishing gradient problem in deep neural networks through skip connections, enabling the construction of deeper, more accurate models. The author explains how these skip connections create a more navigable loss surface, facilitating effective training of the network.</p> <p>Additionally, the author discusses Densely Connected Convolutional Networks (DenseNets) and their dense blocks, which utilize tensor concatenation to allow computation to bypass larger sections of the architecture. While DenseBlocks can be memory-intensive, they are particularly effective for smaller datasets. The article also covers the U-Net architecture, originally developed for biomedical image segmentation, highlighting its effectiveness in tasks requiring outputs of similar size to the inputs.</p> <p>Read the full article here:</p> <p>U-Nets with ResNet Encoders and cross connections</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/page/2/","title":"Blog","text":""}]}